{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-09T16:06:54.029420Z",
     "start_time": "2018-06-09T16:06:53.588751Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import langid\n",
    "import os\n",
    "import itertools\n",
    "from operator import itemgetter\n",
    "import networkx as nx\n",
    "import stemming.porter2 as porter\n",
    "from tokenizer import StanfordCoreNlpTokenizer\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import requests\n",
    "\n",
    "path_or_host = 'http://localhost'\n",
    "url = path_or_host + ':' + str(9000)\n",
    "stanford_core = StanfordCoreNLP(path_or_host)\n",
    "tokenizer = StanfordCoreNlpTokenizer(path_or_host)\n",
    "stem = porter.stem\n",
    "\n",
    "def stanford_service_request(annotators=None, data=None, *args, **kwargs):\n",
    "    if sys.version_info.major >= 3:\n",
    "        data = data.encode('utf-8')\n",
    "\n",
    "    properties = {'annotators': annotators, 'outputFormat': 'json'}\n",
    "    params = {'properties': str(properties), 'pipelineLanguage': 'en'}\n",
    "    if 'pattern' in kwargs:\n",
    "        params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': 'en', 'timeout' : 180000}\n",
    "    r = requests.post(url, params=params, data=data, headers={'Connection': 'close'}, timeout=180000)\n",
    "    r_dict = json.loads(r.text)\n",
    "    return r_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-09T16:07:09.203058Z",
     "start_time": "2018-06-09T16:07:07.015569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nus   183\n",
      "nus total_stories:  183 , stories with entities:  183\n",
      "dict_keys(['headline', 'body', 'entities', 'stemmed_content'])\n",
      "nus\n",
      "nus story 182/183 [len 59812] [entities: 10]\n",
      "nus\n",
      "dep type coverage: 1.000\n",
      "level coverage   : 0.995\n",
      "pos   coverage   : 1.000\n",
      "\n",
      "det\n",
      "cc:preconj\n",
      "compound:prt\n",
      "nummod\n",
      "parataxis\n",
      "auxpass\n",
      "compound\n",
      "neg\n",
      "nmod\n",
      "aux\n",
      "appos\n",
      "dep\n",
      "dobj\n",
      "cc\n",
      "acl:relcl\n",
      "nmod:npmod\n",
      "ccomp\n",
      "ROOT\n",
      "advmod\n",
      "punct\n",
      "mark\n",
      "mwe\n",
      "cop\n",
      "nsubj\n",
      "acl\n",
      "root\n",
      "det:predet\n",
      "nmod:tmod\n",
      "case\n",
      "xcomp\n",
      "nmod:poss\n",
      "conj\n",
      "amod\n",
      "advcl\n",
      "csubj\n",
      "discourse\n",
      "iobj\n",
      "nsubjpass\n",
      "csubjpass\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "datasets_dir = '../datasets'\n",
    "datanames = ['nus']\n",
    "storiesMap = {}\n",
    "\n",
    "for dataset in datanames:\n",
    "    infilename  = os.path.join(datasets_dir, '%s-standard.json' % dataset)\n",
    "    with open(infilename) as f:\n",
    "        stories = json.loads(f.read())\n",
    "    storiesMap[dataset] = stories\n",
    "    \n",
    "for d, v in storiesMap.items():\n",
    "    print(d, ' ', len(v))\n",
    "    \n",
    "for dataset, stories in storiesMap.items():\n",
    "    stories_ = [story for story in stories if len(story['entities']) > 0]\n",
    "    print(dataset, 'total_stories: ' , len(stories), ', stories with entities: ', len(stories_))\n",
    "    print(stories[0].keys())\n",
    "    \n",
    "for dataset, stories in storiesMap.items():\n",
    "    print(dataset)\n",
    "    dataset_dir = os.path.join('../datasets', dataset + '_post_processed')\n",
    "    if not os.path.isdir(dataset_dir): \n",
    "        os.mkdir(dataset_dir)\n",
    "        \n",
    "    for idx, story in enumerate(stories):\n",
    "        story_file = os.path.join(dataset_dir, str(idx) + '.json')\n",
    "        if os.path.isfile(story_file):\n",
    "            with open(story_file) as f:\n",
    "                story_ = json.loads(f.read())\n",
    "                if len(story_['entities']) > 0:\n",
    "                    story = story_\n",
    "        if not os.path.isfile(story_file) or len(story_['entities']) == 0:\n",
    "            content = story['headline'] + '\\n' + story['body']\n",
    "            story['content'] = content\n",
    "            story['suid'] = idx\n",
    "            \n",
    "            # stem story content\n",
    "            original_words, phrases = tokenizer.tokenize(content)\n",
    "            stemmed_words = [stem(word).lower() for word in original_words]\n",
    "            unique_word_list = set(stemmed_words)\n",
    "            story['original_words'] = original_words\n",
    "            story['stemmed_words'] = stemmed_words\n",
    "            story['phrases'] = phrases\n",
    "            story['unique_word_list'] = list(unique_word_list)\n",
    "            \n",
    "            # parse story content\n",
    "            try:\n",
    "                dep = stanford_service_request('depparse', content)\n",
    "                sentences = dep['sentences']\n",
    "            except:\n",
    "                print('\\nfailed processing: ', idx, ' len: ', len(content))\n",
    "                story['body'] = story['headline'] = ''\n",
    "                story['entities'] = []\n",
    "                stories[idx] = story\n",
    "                continue \n",
    "                \n",
    "            word_parsing_levels = {}\n",
    "            dep_types_counter = {}\n",
    "            for sentence in sentences:\n",
    "                dependencies = sentence['basicDependencies']\n",
    "                dep_to_gov_map = {}\n",
    "                for dep in dependencies:\n",
    "                    dep_to_gov_map[dep['dependent']] = dep['governor']\n",
    "                    dep_name = stem(dep['dependentGloss']).lower()\n",
    "                    if dep_name not in unique_word_list:\n",
    "                        continue\n",
    "                    dep_type = dep['dep']\n",
    "                    if dep_name not in dep_types_counter:\n",
    "                        dep_types_counter[dep_name] = {}\n",
    "                    if dep_type not in dep_types_counter[dep_name]:\n",
    "                        dep_types_counter[dep_name][dep_type] = 1\n",
    "                    else:\n",
    "                        dep_types_counter[dep_name][dep_type] += 1\n",
    "                dep_level = {}\n",
    "                def get_level(token_idx):\n",
    "                    if token_idx == 0: return 0\n",
    "                    if token_idx in dep_level: return dep_level[token_idx]\n",
    "                    level = 1 + get_level(dep_to_gov_map[token_idx])\n",
    "                    dep_level[token_idx] = level\n",
    "                    return level\n",
    "                        \n",
    "                for token in sentence['tokens']:\n",
    "                    word = stem(token['originalText']).lower()\n",
    "                    if word not in unique_word_list:\n",
    "                        continue\n",
    "                    level = get_level(token['index'])\n",
    "                    if word not in word_parsing_levels:\n",
    "                        word_parsing_levels[word] = [level]\n",
    "                    else:\n",
    "                        word_parsing_levels[word].append(level)\n",
    "            story['dep_types_counter'] = dep_types_counter\n",
    "            for w, l in word_parsing_levels.items():\n",
    "                word_parsing_levels[w] = sorted(l)\n",
    "            story['word_parsing_levels'] = word_parsing_levels\n",
    "            \n",
    "            # all words position\n",
    "            word_positions = {}\n",
    "            for i, word in enumerate(stemmed_words):\n",
    "                pos = i+1\n",
    "                if word not in word_positions:\n",
    "                    word_positions[word] = [pos]\n",
    "                else:\n",
    "                    word_positions[word].append(pos)\n",
    "            story['word_positions'] = word_positions\n",
    "            \n",
    "            with open(story_file, 'w') as f:\n",
    "                f.write(json.dumps(story))\n",
    "        stories[idx] = story\n",
    "        sys.stdout.write('\\r%s story %d/%d [len %d] [entities: %d]'% (dataset, idx, len(stories), len(story['content']), len(story['entities'])))\n",
    "    print('')\n",
    "for dataset, stories in storiesMap.items():\n",
    "    print(dataset)\n",
    "    level = 0\n",
    "    pos   = 0\n",
    "    dep   = 0\n",
    "    stemmed = 0\n",
    "    for story in stories:\n",
    "        stemmed_words = story['stemmed_words']\n",
    "        level_ = len([s for s in stemmed_words if s in story['word_parsing_levels']])\n",
    "        dep_   = len([s for s in stemmed_words if s in story['dep_types_counter']])\n",
    "        pos_   = len([s for s in stemmed_words if s in story['word_positions']])\n",
    "        stemmed += len(stemmed_words)\n",
    "        level += level_\n",
    "        pos += pos_\n",
    "        dep += dep_\n",
    "    print('dep type coverage: %.3f' % (dep / stemmed))\n",
    "    print('level coverage   : %.3f' % (level / stemmed))\n",
    "    print('pos   coverage   : %.3f' % (pos / stemmed))\n",
    "    print('')\n",
    "    \n",
    "    grammar_parts = set()\n",
    "    for story in stories:\n",
    "        for w, keys in story['dep_types_counter'].items():\n",
    "            grammar_parts.update(keys)\n",
    "    for part in grammar_parts:\n",
    "        print(part)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-09T16:07:32.155755Z",
     "start_time": "2018-06-09T16:07:31.466222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model:  nus\n"
     ]
    }
   ],
   "source": [
    "import boto\n",
    "from gensim.models import Word2Vec\n",
    "#https://github.com/olivettigroup/materials-word-embeddings\n",
    "# word2vec_pretrained = Word2Vec.load(\"../materials-word-embeddings/bin/word2vec_embeddings-SNAPSHOT.model\")\n",
    "\n",
    "word2vec_models_map = {}\n",
    "for dataset in storiesMap.keys():\n",
    "    infile = os.path.join('word2vec_models', dataset)\n",
    "    model = Word2Vec.load(infile)\n",
    "    word2vec_models_map[dataset] = model\n",
    "    print('loaded model: ', dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset, stories in storiesMap.items():\n",
    "    print(dataset.upper())\n",
    "\n",
    "    try:\n",
    "        clf, normalizer_edge, names_edge = linear_models_map[dataset]\n",
    "    except:\n",
    "        print('Faild loading linear model for dataset: ', dataset)\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        clf_u, normalizer, names = linear_p_models_map[dataset]\n",
    "        print('p names', names)\n",
    "    except:\n",
    "        print('Faild loading linear P model for dataset: ', dataset)\n",
    "#         continue\n",
    "        \n",
    "    try:\n",
    "        word2vec_model = word2vec_models_map[dataset]\n",
    "    except:\n",
    "        print('Failed loading word2vec model for dataset', dataset)\n",
    "        continue\n",
    "\n",
    "    avg_time = 0\n",
    "    GR = []\n",
    "    PRED = []\n",
    "    \n",
    "    all_stories = [s.copy() for s in stories]\n",
    "    all_stories = sorted(all_stories, key = lambda e : str(e['suid']))\n",
    "    len_all = len(all_stories)\n",
    "    len_train = round(0.8 * len_all)\n",
    "    random.seed(10)\n",
    "    random.shuffle(all_stories)\n",
    "    \n",
    "    train_stories = all_stories[:len_train]\n",
    "    test_stories = all_stories[len_train:]\n",
    "    print('TEST  stories [%d]' % len(test_stories))\n",
    "    print([story['suid'] for story in test_stories])\n",
    "    print('')\n",
    "    \n",
    "    for story_idx, story in enumerate(test_stories):\n",
    "        if len(story['body']) == 0: continue\n",
    "        start_time = time.time()\n",
    "        processed = False\n",
    "\n",
    "        phrases = story['phrases']\n",
    "        stemmed_words = story['stemmed_words']\n",
    "        word_positions = story['word_positions']\n",
    "        word_levels = story['word_parsing_levels']\n",
    "        dep_types_counter = story['dep_types_counter']\n",
    "        \n",
    "        entities = story['entities']\n",
    "        gold_grams = set()\n",
    "        for entity in entities:\n",
    "            grams = stanford_core.word_tokenize(entity['id'])\n",
    "            grams = [stem(token).lower() for token in grams]\n",
    "            gold_grams.update(grams)\n",
    "            \n",
    "        # compute feature pre-reqs\n",
    "        word2vec = {}\n",
    "        for i, word in enumerate(stemmed_words):\n",
    "            if word in word2vec.keys() or word not in word2vec_model.wv.vocab:\n",
    "                continue\n",
    "            w_embed = word2vec_model.wv.get_vector(word)\n",
    "            word2vec[word] = w_embed\n",
    "        count = {}\n",
    "        pos = {}\n",
    "        first_pos = {}\n",
    "        \n",
    "        def update_or_insert(d, k, v):\n",
    "            if k in d.keys():\n",
    "                d[k] += v\n",
    "            else:\n",
    "                d[k] = v\n",
    "\n",
    "        for i, word in enumerate(stemmed_words):\n",
    "            update_or_insert(count, word, 1)\n",
    "            update_or_insert(pos, word, 1 / (i + 1))\n",
    "            if not word in first_pos:\n",
    "                first_pos[word] = 1/(i+1)\n",
    "                \n",
    "            \n",
    "        def get_part_strength(d, key):\n",
    "            def get_count(k):\n",
    "                if k in d:\n",
    "                    return d[k]\n",
    "                else:\n",
    "                    return 0\n",
    "            s = sum([get_count(k) for k in d.keys() if key in k.lower()])\n",
    "            return s\n",
    "        \n",
    "        def compose_features(u, v, H, H_first):\n",
    "            count_u = count[u]\n",
    "            count_v = count[v]\n",
    "            pos_u = pos[u]\n",
    "            pos_v = pos[v]\n",
    "            wvec_u = word2vec[u]\n",
    "            wvec_v = word2vec[v]\n",
    "            u_level = 1 - 1 / np.mean(word_levels[u])\n",
    "            v_level = 1 - 1 / np.mean(word_levels[v])\n",
    "            s_uv = word2vec_model.wv.similarity(u, v)\n",
    "            \n",
    "#             parts = [\n",
    "#                 'subj',\n",
    "#                 'obj',\n",
    "#                 'appos',\n",
    "#                 'conj',\n",
    "#                 'compound',\n",
    "#                 'mod',\n",
    "#                 'det',\n",
    "#                 ]                        \n",
    "#             # part of sentence\n",
    "            parts = [\n",
    "                'subj',\n",
    "                'obj',\n",
    "                'appos',\n",
    "                'conj',\n",
    "                'compound',\n",
    "                'mod',\n",
    "                'acl',\n",
    "                'det',\n",
    "                'neg',\n",
    "                'mark',\n",
    "                'auxpass',\n",
    "                'ccomp',\n",
    "                'mwe',\n",
    "                'parataxis',\n",
    "                'cop',\n",
    "                'advcl',\n",
    "                'cc',\n",
    "                'discourse',\n",
    "                'xcomp',\n",
    "                'case',\n",
    "                'dep',\n",
    "                'aux',\n",
    "                'punct',\n",
    "                'root',\n",
    "            ]\n",
    "#             parts = [\n",
    "#                 'csubj',\n",
    "#                 'nsubj',\n",
    "#                 'nsubjpass',\n",
    "#                 'csubjpass',\n",
    "#                 'dobj',\n",
    "#                 'iobj',\n",
    "#                 'appos',\n",
    "#                 'conj',\n",
    "#                 'cc:preconj',\n",
    "#                 'compound',\n",
    "#                 'compound:prt',\n",
    "#                 'nmod',\n",
    "#                 'advmod',\n",
    "#                 'nummod',\n",
    "#                 'nmod:tmod',\n",
    "#                 'nmod:npmod',\n",
    "#                 'nmod:poss',\n",
    "#                 'amod',\n",
    "#                 'acl',\n",
    "#                 'acl:relcl',\n",
    "#                 'det',\n",
    "#                 'det:predet',\n",
    "#                 'neg',\n",
    "#                 'mark',\n",
    "#                 'auxpass',\n",
    "#                 'ccomp',\n",
    "#                 'mwe',\n",
    "#                 'parataxis',\n",
    "#                 'cop',\n",
    "#                 'advcl',\n",
    "#                 'cc',\n",
    "#                 'discourse',\n",
    "#                 'xcomp',\n",
    "#                 'case',\n",
    "#                 'dep',\n",
    "#                 'root',\n",
    "#                 'aux',\n",
    "#                 'punct',\n",
    "#             ]\n",
    "            v_parts = np.array([get_part_strength(dep_types_counter[v], part) for part in parts])\n",
    "            u_parts = np.array([get_part_strength(dep_types_counter[u], part) for part in parts])\n",
    "\n",
    "            # co-occurence count\n",
    "            co_occur = H[u][v]['weight']\n",
    "\n",
    "            # first time together\n",
    "            co_first = H_first[u][v]['weight']\n",
    "\n",
    "            # neighbours\n",
    "            u_neigs = list(H[u].keys())\n",
    "            v_neigs = list(H[v].keys())\n",
    "            uv_neigs = [u for u in u_neigs if u in v_neigs]\n",
    "\n",
    "            x = np.concatenate(\n",
    "                (np.array([\n",
    "                    count_u, count_v, pos_u, pos_v, u_level, v_level, s_uv,\n",
    "                    co_occur,\n",
    "                    co_first,\n",
    "                    len(u_neigs),\n",
    "                    len(v_neigs),\n",
    "                    len(uv_neigs),\n",
    "                ]), u_parts, v_parts, wvec_u, wvec_v),\n",
    "                axis=0)\n",
    "            x = np.expand_dims(x, axis=0)\n",
    "            return x\n",
    "\n",
    "        def valid_token(u, verbose=False):\n",
    "            if u not in word_levels.keys():\n",
    "                if verbose:\n",
    "                    print(u, 'not in word_levels')\n",
    "                return False\n",
    "            if u not in word2vec_model.wv.vocab:\n",
    "                if verbose:\n",
    "                    print(u, 'not in vocabulary')\n",
    "                return False\n",
    "            return True\n",
    "        \n",
    "        H_first = nx.Graph()\n",
    "        H_all = nx.Graph()\n",
    "        for i, v in enumerate(stemmed_words):\n",
    "            if len(v) <= 1 or not valid_token(v):\n",
    "                continue\n",
    "            for offset in window_offsets:\n",
    "                idx = i + offset\n",
    "                if idx < len(stemmed_words) and idx >= 0:\n",
    "                    u = stemmed_words[idx]\n",
    "                    if u == v:\n",
    "                        continue\n",
    "                    if len(u) <= 1 or not valid_token(u):\n",
    "                        continue\n",
    "                    if H_all.has_edge(u, v):\n",
    "                        H_all[u][v]['weight'] += 1\n",
    "                    else:\n",
    "                        H_all.add_edge(u, v, weight=1)\n",
    "                        assert(H_first.has_edge(u, v) == False)\n",
    "                        H_first.add_edge(u, v, weight=1/(i+1))\n",
    "        P_all = {}\n",
    "        for v in H_all:\n",
    "            if v not in P_all.keys():\n",
    "                P_all[v] = pos[v]\n",
    "                \n",
    "        # create edges       \n",
    "        H = nx.DiGraph()\n",
    "        for i, v in enumerate(stemmed_words):\n",
    "            if len(v) <= 1 or not valid_token(v):\n",
    "                continue\n",
    "            for offset in window_offsets:\n",
    "                idx = i + offset\n",
    "                if idx < len(stemmed_words) and idx >= 0:\n",
    "                    u = stemmed_words[idx]\n",
    "                    if u == v:\n",
    "                        continue\n",
    "                    if len(u) <= 1 or not valid_token(u):\n",
    "                        continue\n",
    "                    if H.has_edge(u, v) and H.has_edge(v, u):\n",
    "                        continue                   \n",
    "                    uv = compose_features(u, v, H_all, H_first)\n",
    "                    uv = normalizer_edge.transform(uv)\n",
    "                    uv_s = float(clf.predict(uv))\n",
    "                    if uv_s >= 0.7:\n",
    "                        H.add_edge(u, v, weight=uv_s)\n",
    "                    vu = compose_features(v, u, H_all, H_first)\n",
    "                    vu = normalizer_edge.transform(vu)\n",
    "                    vu_s = float(clf.predict(vu))\n",
    "                    if vu_s >= 0.7:\n",
    "                        H.add_edge(v, u, weight=vu_s)\n",
    "                    GR.append(int(v in gold_grams))\n",
    "                    GR.append(int(u in gold_grams))\n",
    "                    PRED.append(uv_s)\n",
    "                    PRED.append(vu_s)\n",
    "                                 \n",
    "        P = {}\n",
    "        for v in H:\n",
    "            if v not in P.keys():\n",
    "                P[v] = pos[v]\n",
    "            for u in H[v]:\n",
    "                if u not in P.keys():\n",
    "                    P[u] = pos[u]\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

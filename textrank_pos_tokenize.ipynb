{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:31:33.424551Z",
     "start_time": "2018-06-05T12:31:33.010495Z"
    },
    "code_folding": [],
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import langid\n",
    "import os\n",
    "import itertools\n",
    "from operator import itemgetter\n",
    "import networkx as nx\n",
    "import stemming.porter2 as porter\n",
    "from tokenizer import StanfordCoreNlpTokenizer\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import requests\n",
    "\n",
    "path_or_host = 'http://localhost'\n",
    "url = path_or_host + ':' + str(9000)\n",
    "stanford_core = StanfordCoreNLP(path_or_host)\n",
    "tokenizer = StanfordCoreNlpTokenizer(path_or_host)\n",
    "stem = porter.stem\n",
    "\n",
    "def stanford_service_request(annotators=None, data=None, *args, **kwargs):\n",
    "    if sys.version_info.major >= 3:\n",
    "        data = data.encode('utf-8')\n",
    "\n",
    "    properties = {'annotators': annotators, 'outputFormat': 'json'}\n",
    "    params = {'properties': str(properties), 'pipelineLanguage': 'en'}\n",
    "    if 'pattern' in kwargs:\n",
    "        params = {\"pattern\": kwargs['pattern'], 'properties': str(properties), 'pipelineLanguage': 'en', 'timeout' : 180000}\n",
    "    r = requests.post(url, params=params, data=data, headers={'Connection': 'close'}, timeout=180000)\n",
    "    r_dict = json.loads(r.text)\n",
    "    return r_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:31:58.695957Z",
     "start_time": "2018-06-05T12:31:56.549198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nus   183\n",
      "nus total_stories:  183 , stories with entities:  183\n",
      "dict_keys(['headline', 'body', 'entities', 'stemmed_content'])\n",
      "nus\n",
      "nus story 182/183 [len 59812] [entities: 10]\n",
      "nus\n",
      "dep type coverage: 1.000\n",
      "level coverage   : 0.995\n",
      "pos   coverage   : 1.000\n",
      "\n",
      "advmod\n",
      "csubj\n",
      "neg\n",
      "iobj\n",
      "discourse\n",
      "compound:prt\n",
      "amod\n",
      "punct\n",
      "acl:relcl\n",
      "aux\n",
      "xcomp\n",
      "auxpass\n",
      "nmod:poss\n",
      "dep\n",
      "nmod\n",
      "ROOT\n",
      "nsubjpass\n",
      "nmod:npmod\n",
      "det\n",
      "parataxis\n",
      "nmod:tmod\n",
      "csubjpass\n",
      "compound\n",
      "appos\n",
      "advcl\n",
      "case\n",
      "ccomp\n",
      "nsubj\n",
      "mwe\n",
      "acl\n",
      "det:predet\n",
      "dobj\n",
      "root\n",
      "cc:preconj\n",
      "conj\n",
      "cop\n",
      "mark\n",
      "cc\n",
      "nummod\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "datasets_dir = '../datasets'\n",
    "datanames = ['nus']\n",
    "storiesMap = {}\n",
    "\n",
    "for dataset in datanames:\n",
    "    infilename  = os.path.join(datasets_dir, '%s-standard.json' % dataset)\n",
    "    with open(infilename) as f:\n",
    "        stories = json.loads(f.read())\n",
    "    storiesMap[dataset] = stories\n",
    "    \n",
    "for d, v in storiesMap.items():\n",
    "    print(d, ' ', len(v))\n",
    "    \n",
    "for dataset, stories in storiesMap.items():\n",
    "    stories_ = [story for story in stories if len(story['entities']) > 0]\n",
    "    print(dataset, 'total_stories: ' , len(stories), ', stories with entities: ', len(stories_))\n",
    "    print(stories[0].keys())\n",
    "    \n",
    "for dataset, stories in storiesMap.items():\n",
    "    print(dataset)\n",
    "    dataset_dir = os.path.join('../datasets', dataset + '_post_processed')\n",
    "    if not os.path.isdir(dataset_dir): \n",
    "        os.mkdir(dataset_dir)\n",
    "        \n",
    "    for idx, story in enumerate(stories):\n",
    "        story_file = os.path.join(dataset_dir, str(idx) + '.json')\n",
    "        if os.path.isfile(story_file):\n",
    "            with open(story_file) as f:\n",
    "                story_ = json.loads(f.read())\n",
    "                if len(story_['entities']) > 0:\n",
    "                    story = story_\n",
    "        if not os.path.isfile(story_file) or len(story_['entities']) == 0:\n",
    "            content = story['headline'] + '\\n' + story['body']\n",
    "            story['content'] = content\n",
    "            story['suid'] = idx\n",
    "            \n",
    "            # stem story content\n",
    "            original_words, phrases = tokenizer.tokenize(content)\n",
    "            stemmed_words = [stem(word).lower() for word in original_words]\n",
    "            unique_word_list = set(stemmed_words)\n",
    "            story['original_words'] = original_words\n",
    "            story['stemmed_words'] = stemmed_words\n",
    "            story['phrases'] = phrases\n",
    "            story['unique_word_list'] = list(unique_word_list)\n",
    "            \n",
    "            # parse story content\n",
    "            try:\n",
    "                dep = stanford_service_request('depparse', content)\n",
    "                sentences = dep['sentences']\n",
    "            except:\n",
    "                print('\\nfailed processing: ', idx, ' len: ', len(content))\n",
    "                story['body'] = story['headline'] = ''\n",
    "                story['entities'] = []\n",
    "                stories[idx] = story\n",
    "                continue \n",
    "                \n",
    "            word_parsing_levels = {}\n",
    "            dep_types_counter = {}\n",
    "            for sentence in sentences:\n",
    "                dependencies = sentence['basicDependencies']\n",
    "                dep_to_gov_map = {}\n",
    "                for dep in dependencies:\n",
    "                    dep_to_gov_map[dep['dependent']] = dep['governor']\n",
    "                    dep_name = stem(dep['dependentGloss']).lower()\n",
    "                    if dep_name not in unique_word_list:\n",
    "                        continue\n",
    "                    dep_type = dep['dep']\n",
    "                    if dep_name not in dep_types_counter:\n",
    "                        dep_types_counter[dep_name] = {}\n",
    "                    if dep_type not in dep_types_counter[dep_name]:\n",
    "                        dep_types_counter[dep_name][dep_type] = 1\n",
    "                    else:\n",
    "                        dep_types_counter[dep_name][dep_type] += 1\n",
    "                dep_level = {}\n",
    "                def get_level(token_idx):\n",
    "                    if token_idx == 0: return 0\n",
    "                    if token_idx in dep_level: return dep_level[token_idx]\n",
    "                    level = 1 + get_level(dep_to_gov_map[token_idx])\n",
    "                    dep_level[token_idx] = level\n",
    "                    return level\n",
    "                        \n",
    "                for token in sentence['tokens']:\n",
    "                    word = stem(token['originalText']).lower()\n",
    "                    if word not in unique_word_list:\n",
    "                        continue\n",
    "                    level = get_level(token['index'])\n",
    "                    if word not in word_parsing_levels:\n",
    "                        word_parsing_levels[word] = [level]\n",
    "                    else:\n",
    "                        word_parsing_levels[word].append(level)\n",
    "            story['dep_types_counter'] = dep_types_counter\n",
    "            for w, l in word_parsing_levels.items():\n",
    "                word_parsing_levels[w] = sorted(l)\n",
    "            story['word_parsing_levels'] = word_parsing_levels\n",
    "            \n",
    "            # all words position\n",
    "            word_positions = {}\n",
    "            for i, word in enumerate(stemmed_words):\n",
    "                pos = i+1\n",
    "                if word not in word_positions:\n",
    "                    word_positions[word] = [pos]\n",
    "                else:\n",
    "                    word_positions[word].append(pos)\n",
    "            story['word_positions'] = word_positions\n",
    "            \n",
    "            with open(story_file, 'w') as f:\n",
    "                f.write(json.dumps(story))\n",
    "        stories[idx] = story\n",
    "        sys.stdout.write('\\r%s story %d/%d [len %d] [entities: %d]'% (dataset, idx, len(stories), len(story['content']), len(story['entities'])))\n",
    "    print('')\n",
    "for dataset, stories in storiesMap.items():\n",
    "    print(dataset)\n",
    "    level = 0\n",
    "    pos   = 0\n",
    "    dep   = 0\n",
    "    stemmed = 0\n",
    "    for story in stories:\n",
    "        stemmed_words = story['stemmed_words']\n",
    "        level_ = len([s for s in stemmed_words if s in story['word_parsing_levels']])\n",
    "        dep_   = len([s for s in stemmed_words if s in story['dep_types_counter']])\n",
    "        pos_   = len([s for s in stemmed_words if s in story['word_positions']])\n",
    "        stemmed += len(stemmed_words)\n",
    "        level += level_\n",
    "        pos += pos_\n",
    "        dep += dep_\n",
    "    print('dep type coverage: %.3f' % (dep / stemmed))\n",
    "    print('level coverage   : %.3f' % (level / stemmed))\n",
    "    print('pos   coverage   : %.3f' % (pos / stemmed))\n",
    "    print('')\n",
    "    \n",
    "    grammar_parts = set()\n",
    "    for story in stories:\n",
    "        for w, keys in story['dep_types_counter'].items():\n",
    "            grammar_parts.update(keys)\n",
    "    for part in grammar_parts:\n",
    "        print(part)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-03T08:27:48.370226Z",
     "start_time": "2018-06-03T08:27:47.697710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('159', 'ROOT')]\n",
      "servic 26\n",
      "   dobj 34\n",
      "   compound 95\n",
      "   nmod 100\n",
      "   nsubj 25\n",
      "   appos 2\n",
      "   nsubjpass 1\n",
      "   ROOT 3\n",
      "   dep 1\n",
      "   root 1\n",
      "   conj 2\n",
      "term 17\n",
      "   nmod 77\n",
      "   compound 50\n",
      "   nsubj 12\n",
      "   dep 9\n",
      "   dobj 25\n",
      "   root 3\n",
      "   nsubjpass 5\n",
      "   appos 2\n",
      "   conj 1\n",
      "set 14\n",
      "   advcl 1\n",
      "   nmod 22\n",
      "   dep 1\n",
      "   appos 1\n",
      "   dobj 12\n",
      "   amod 5\n",
      "   nsubjpass 5\n",
      "   ROOT 1\n",
      "   nsubj 9\n",
      "   conj 1\n",
      "   compound 1\n",
      "prober 14\n",
      "   nmod 17\n",
      "   xcomp 1\n",
      "   nsubj 14\n",
      "   dep 3\n",
      "   conj 3\n",
      "   dobj 6\n",
      "   nmod:poss 1\n",
      "   compound 1\n",
      "document 12\n",
      "   dobj 22\n",
      "   nmod 52\n",
      "   nsubj 6\n",
      "   conj 3\n",
      "   compound 12\n",
      "   appos 1\n",
      "   nsubjpass 6\n",
      "   dep 2\n",
      "   ROOT 1\n",
      "j 10\n",
      "   dep 12\n",
      "   compound 9\n",
      "   conj 9\n",
      "   amod 1\n",
      "   nmod 11\n",
      "   nsubj 9\n",
      "   appos 20\n",
      "   dobj 3\n",
      "   nsubjpass 1\n",
      "summari 9\n",
      "   compound 12\n",
      "   dep 3\n",
      "   xcomp 2\n",
      "   nsubj 8\n",
      "   dobj 21\n",
      "   nmod 15\n",
      "   conj 1\n",
      "   nsubjpass 1\n",
      "approach 7\n",
      "   dep 2\n",
      "   nsubj 5\n",
      "   nsubjpass 2\n",
      "   dobj 1\n",
      "   nmod 1\n",
      "   ROOT 1\n",
      "techniqu 7\n",
      "   conj 1\n",
      "   dobj 10\n",
      "   nsubjpass 3\n",
      "   nmod 13\n",
      "   nsubj 4\n",
      "   dep 1\n",
      "   ROOT 1\n",
      "relationship 7\n",
      "   nmod 10\n",
      "   dobj 12\n",
      "   compound 23\n",
      "   nsubj 5\n",
      "   nsubjpass 2\n",
      "   conj 1\n",
      "algorithm 7\n",
      "   nmod 8\n",
      "   nsubj 6\n",
      "   dobj 5\n",
      "   compound 2\n",
      "   nsubjpass 1\n",
      "   ROOT 1\n",
      "probe 6\n",
      "   nmod 28\n",
      "   advcl 8\n",
      "   acl 15\n",
      "   amod 29\n",
      "   nsubj 4\n",
      "   dobj 9\n",
      "   dep 7\n",
      "   ROOT 6\n",
      "   compound 12\n",
      "   conj 1\n",
      "   nsubjpass 2\n",
      "research 6\n",
      "   nsubjpass 1\n",
      "   compound 2\n",
      "   nsubj 5\n",
      "   conj 1\n",
      "   root 1\n",
      "   ROOT 1\n",
      "k 6\n",
      "   nmod 4\n",
      "   nsubj 5\n",
      "   compound 19\n",
      "   dobj 2\n",
      "   dep 5\n",
      "   nsubjpass 1\n",
      "   appos 1\n",
      "group 5\n",
      "   nmod 23\n",
      "   nsubj 2\n",
      "   conj 1\n",
      "   compound 2\n",
      "   dep 3\n",
      "   nsubjpass 3\n",
      "   acl 1\n",
      "   dobj 3\n",
      "   ROOT 3\n",
      "   amod 1\n",
      "goal 5\n",
      "   nsubj 5\n",
      "   dobj 1\n",
      "condit 5\n",
      "   nsubjpass 2\n",
      "   nmod 2\n",
      "   dobj 2\n",
      "   dep 1\n",
      "   nsubj 3\n",
      "   compound 1\n",
      "basil 4\n",
      "   nmod 6\n",
      "   xcomp 3\n",
      "   nsubj 4\n",
      "   compound 6\n",
      "   dep 1\n",
      "   dobj 1\n",
      "   nmod:poss 2\n",
      "type 4\n",
      "   dobj 4\n",
      "   nmod 3\n",
      "   acl:relcl 1\n",
      "   nsubj 4\n",
      "   conj 1\n",
      "   advcl 1\n",
      "   ROOT 1\n",
      "exampl 4\n",
      "   nmod 7\n",
      "   dobj 4\n",
      "   nsubj 4\n",
      "   ROOT 2\n",
      "   dep 1\n",
      "   compound 1\n",
      "googl 4\n",
      "   conj 2\n",
      "   compound 3\n",
      "   dep 1\n",
      "   nsubj 4\n",
      "   nmod 3\n",
      "   appos 3\n",
      "   ROOT 2\n",
      "queri 4\n",
      "   advcl 1\n",
      "   compound 32\n",
      "   nmod 11\n",
      "   nsubj 4\n",
      "   dobj 4\n",
      "   xcomp 2\n",
      "   ROOT 1\n",
      "   acl 1\n",
      "   amod 1\n",
      "   conj 1\n",
      "user 4\n",
      "   nsubj 4\n",
      "erm 4\n",
      "   nmod 5\n",
      "   nsubj 4\n",
      "   dep 4\n",
      "   compound 3\n",
      "   dobj 2\n",
      "focus 3\n",
      "   compound 32\n",
      "   nmod 6\n",
      "   dobj 5\n",
      "   advcl 3\n",
      "   amod 1\n",
      "   conj 1\n",
      "   nsubj 3\n",
      "   ccomp 2\n",
      "   ROOT 1\n",
      "experi 3\n",
      "   nmod 9\n",
      "   dobj 2\n",
      "   conj 2\n",
      "   compound 3\n",
      "   nsubj 2\n",
      "   dep 1\n",
      "   nsubjpass 1\n",
      "site 3\n",
      "   conj 3\n",
      "   nsubj 3\n",
      "   dobj 10\n",
      "   nmod 6\n",
      "   dep 1\n",
      "   appos 4\n",
      "method 3\n",
      "   nsubj 3\n",
      "   ROOT 1\n",
      "number 3\n",
      "   nmod 13\n",
      "   dobj 6\n",
      "   nsubj 3\n",
      "   parataxis 1\n",
      "   dep 1\n",
      "   conj 1\n",
      "f 3\n",
      "   compound 26\n",
      "   ccomp 1\n",
      "   nmod 3\n",
      "   nsubj 3\n",
      "   dep 4\n",
      "   amod 1\n",
      "cosin 3\n",
      "   compound 7\n",
      "   nsubj 3\n",
      "framework 3\n",
      "   nsubj 3\n",
      "   dobj 2\n",
      "jl 3\n",
      "   nsubj 3\n",
      "   compound 1\n",
      "c 3\n",
      "   nsubj 3\n",
      "   dep 1\n",
      "   compound 15\n",
      "   nmod 2\n",
      "prototyp 2\n",
      "   nsubj 2\n",
      "   nmod 2\n",
      "   conj 2\n",
      "source-bias 2\n",
      "   amod 82\n",
      "   xcomp 4\n",
      "   dep 3\n",
      "   nsubj 2\n",
      "   dobj 1\n",
      "   advmod 1\n",
      "   nmod 1\n",
      "sourc 2\n",
      "   compound 31\n",
      "   xcomp 1\n",
      "   nmod 30\n",
      "   dep 2\n",
      "   dobj 4\n",
      "   appos 1\n",
      "   nsubjpass 1\n",
      "   nsubj 1\n",
      "system 2\n",
      "   nmod 4\n",
      "   dobj 1\n",
      "   nsubj 2\n",
      "   ROOT 1\n",
      "   dep 1\n",
      "copi 2\n",
      "   dobj 1\n",
      "   nsubjpass 1\n",
      "   nsubj 1\n",
      "   ROOT 1\n",
      "collect 2\n",
      "   root 1\n",
      "   dobj 4\n",
      "   acl 1\n",
      "   nmod 13\n",
      "   appos 2\n",
      "   nsubj 2\n",
      "   compound 2\n",
      "   ROOT 2\n",
      "   conj 2\n",
      "   advcl 1\n",
      "need 2\n",
      "   nsubj 2\n",
      "   parataxis 2\n",
      "   ROOT 2\n",
      "mechan 2\n",
      "   nmod 4\n",
      "   nsubj 2\n",
      "   dobj 2\n",
      "   ROOT 1\n",
      "cluster 2\n",
      "   nmod 9\n",
      "   compound 16\n",
      "   advcl 1\n",
      "   dobj 2\n",
      "   dep 1\n",
      "   nsubj 2\n",
      "similar 2\n",
      "   amod 17\n",
      "   advmod 3\n",
      "   dobj 1\n",
      "   nmod 1\n",
      "   nsubjpass 2\n",
      "   compound 6\n",
      "   ccomp 2\n",
      "   dep 1\n",
      "   ROOT 2\n",
      "   appos 2\n",
      "weight 2\n",
      "   nmod 6\n",
      "   dobj 7\n",
      "   amod 3\n",
      "   nsubjpass 2\n",
      "   conj 4\n",
      "   ROOT 2\n",
      "   compound 2\n",
      "   acl:relcl 1\n",
      "   dep 3\n",
      "fraction 2\n",
      "   nsubj 1\n",
      "   nmod 1\n",
      "   nsubjpass 1\n",
      "   dobj 1\n",
      "altern 2\n",
      "   amod 3\n",
      "   nmod 1\n",
      "   nsubj 2\n",
      "   conj 1\n",
      "way 2\n",
      "   nmod 2\n",
      "   nsubj 2\n",
      "   conj 1\n",
      "   ROOT 1\n",
      "ik 2\n",
      "   dobj 1\n",
      "   nsubj 2\n",
      "   compound 1\n",
      "   dep 1\n",
      "esummari 2\n",
      "   conj 2\n",
      "   nsubj 2\n",
      "   xcomp 1\n",
      "   dobj 1\n",
      "   nmod 14\n",
      "   dep 5\n",
      "   compound 1\n",
      "   appos 1\n",
      "   ROOT 2\n",
      "   amod 1\n",
      "bia 2\n",
      "   dep 3\n",
      "   compound 23\n",
      "   xcomp 1\n",
      "   appos 5\n",
      "   nmod 3\n",
      "   dobj 1\n",
      "   conj 1\n",
      "   nsubj 2\n",
      "figur 2\n",
      "   advcl 1\n",
      "   nsubj 2\n",
      "   dep 6\n",
      "   dobj 1\n",
      "   nmod 8\n",
      "   compound 1\n",
      "   nmod:tmod 1\n",
      "function 2\n",
      "   nmod 3\n",
      "   conj 2\n",
      "   nsubjpass 1\n",
      "   dobj 1\n",
      "   nsubj 1\n",
      "valu 2\n",
      "   dobj 1\n",
      "   nmod 7\n",
      "   appos 2\n",
      "   conj 1\n",
      "   nsubj 2\n",
      "idea 2\n",
      "   nsubj 2\n",
      "l 2\n",
      "   nmod 3\n",
      "   dep 1\n",
      "   dobj 1\n",
      "   nsubj 2\n",
      "   compound 1\n",
      "center 2\n",
      "   dobj 2\n",
      "   dep 2\n",
      "   nsubj 2\n",
      "   nmod 3\n",
      "   ccomp 1\n",
      "   ROOT 1\n",
      "h 2\n",
      "   nsubj 2\n",
      "   nmod 1\n",
      "centroid 2\n",
      "   dobj 2\n",
      "   advcl 1\n",
      "   nsubj 2\n",
      "   compound 1\n",
      "web 1\n",
      "   compound 72\n",
      "   nsubj 1\n",
      "   dobj 1\n",
      "paper 1\n",
      "   nmod 5\n",
      "   nsubj 1\n",
      "relev 1\n",
      "   amod 9\n",
      "   compound 20\n",
      "   ccomp 4\n",
      "   nmod 4\n",
      "   dobj 8\n",
      "   conj 1\n",
      "   nsubj 1\n",
      "   dep 3\n",
      "   xcomp 2\n",
      "   ROOT 1\n",
      "metric 1\n",
      "   conj 4\n",
      "   nmod 8\n",
      "   amod 3\n",
      "   dobj 2\n",
      "   nsubj 1\n",
      "precis 1\n",
      "   amod 2\n",
      "   dobj 5\n",
      "   dep 2\n",
      "   nmod 2\n",
      "   nsubj 1\n",
      "   compound 1\n",
      "interest 1\n",
      "   amod 13\n",
      "   dep 1\n",
      "   ROOT 1\n",
      "   advcl 1\n",
      "   advmod 2\n",
      "   nsubj 1\n",
      "perform 1\n",
      "   compound 3\n",
      "   conj 1\n",
      "   nsubj 1\n",
      "   dobj 2\n",
      "   ROOT 1\n",
      "categori 1\n",
      "   ROOT 1\n",
      "   nmod 1\n",
      "   nsubjpass 1\n",
      "   conj 1\n",
      "compani 1\n",
      "   nsubj 1\n",
      "categor 1\n",
      "   ROOT 2\n",
      "   conj 1\n",
      "   nsubj 1\n",
      "   dep 1\n",
      "portion 1\n",
      "   nsubjpass 1\n",
      "   appos 2\n",
      "permiss 1\n",
      "   nsubjpass 1\n",
      "   dobj 1\n",
      "distribut 1\n",
      "   conj 1\n",
      "   dobj 2\n",
      "   amod 2\n",
      "   nsubj 1\n",
      "   nmod 1\n",
      "advantag 1\n",
      "   conj 1\n",
      "   dobj 2\n",
      "   nsubj 1\n",
      "   ccomp 1\n",
      "data 1\n",
      "   compound 14\n",
      "   nmod 6\n",
      "   nsubj 1\n",
      "   dobj 1\n",
      "amazon 1\n",
      "   nsubj 1\n",
      "model 1\n",
      "   nmod 4\n",
      "   conj 1\n",
      "   nsubj 1\n",
      "   dobj 2\n",
      "   acl 1\n",
      "   ROOT 1\n",
      "analysi 1\n",
      "   dobj 2\n",
      "   nmod 2\n",
      "   nsubj 1\n",
      "   dep 2\n",
      "effectiv 1\n",
      "   amod 1\n",
      "   ROOT 1\n",
      "   nsubj 1\n",
      "organ 1\n",
      "   nsubj 1\n",
      "   conj 1\n",
      "   dobj 1\n",
      "former 1\n",
      "   nsubj 1\n",
      "differ 1\n",
      "   amod 9\n",
      "   dobj 2\n",
      "   nsubj 1\n",
      "   conj 2\n",
      "registri 1\n",
      "   nmod 4\n",
      "   nsubj 1\n",
      "   ROOT 1\n",
      "   compound 2\n",
      "one 1\n",
      "   nmod 2\n",
      "   nummod 8\n",
      "   dobj 4\n",
      "   nsubjpass 1\n",
      "question 1\n",
      "   nsubj 1\n",
      "   conj 1\n",
      "request 1\n",
      "   dobj 4\n",
      "   nmod 5\n",
      "   dep 1\n",
      "   amod 1\n",
      "   compound 1\n",
      "   nsubjpass 1\n",
      "mean 1\n",
      "   nsubj 1\n",
      "   ROOT 2\n",
      "natur 1\n",
      "   nmod 1\n",
      "   dobj 4\n",
      "   nsubj 1\n",
      "measur 1\n",
      "   conj 2\n",
      "   advcl 2\n",
      "   dobj 10\n",
      "   ROOT 2\n",
      "   nmod 1\n",
      "   nsubj 1\n",
      "result 1\n",
      "   nsubj 1\n",
      "   nmod 2\n",
      "   ROOT 3\n",
      "   parataxis 1\n",
      "   root 1\n",
      "   ccomp 1\n",
      "   xcomp 1\n",
      "   dobj 1\n",
      "problem 1\n",
      "   conj 1\n",
      "   dobj 2\n",
      "   nmod 2\n",
      "   nsubj 1\n",
      "univers 1\n",
      "   dobj 1\n",
      "   nmod 2\n",
      "   nsubj 1\n",
      "w 1\n",
      "   nmod 5\n",
      "   nsubj 1\n",
      "   compound 36\n",
      "   dep 5\n",
      "   appos 2\n",
      "d 1\n",
      "   compound 13\n",
      "   dep 2\n",
      "   appos 3\n",
      "   nsubj 1\n",
      "   nmod 3\n",
      "m 1\n",
      "   compound 12\n",
      "   appos 5\n",
      "   dobj 1\n",
      "   nsubj 1\n",
      "   dep 4\n",
      "n 1\n",
      "   compound 19\n",
      "   appos 9\n",
      "   dep 6\n",
      "   nsubj 1\n",
      "   nmod 1\n",
      "stopword 1\n",
      "   nsubjpass 1\n",
      "   nmod 1\n",
      "   conj 1\n",
      "optional 1\n",
      "   nsubjpass 1\n",
      "compon 1\n",
      "   dobj 3\n",
      "   acl:relcl 1\n",
      "   nsubj 1\n",
      "jk 1\n",
      "   nsubj 1\n",
      "   dobj 2\n",
      "   compound 1\n",
      "size 1\n",
      "   nsubj 1\n",
      "   nmod 2\n",
      "repeat 1\n",
      "   amod 1\n",
      "   ROOT 1\n",
      "   nsubj 1\n",
      "   compound 1\n",
      "   nmod 1\n",
      "   root 1\n",
      "sampl 1\n",
      "   nmod 1\n",
      "   dep 1\n",
      "   conj 1\n",
      "   nsubj 1\n",
      "   ROOT 2\n",
      "   compound 2\n",
      "less 1\n",
      "   nsubjpass 1\n",
      "   advcl 1\n",
      "   amod 2\n",
      "callan 1\n",
      "   compound 3\n",
      "   amod 1\n",
      "   ROOT 1\n",
      "   nsubj 1\n",
      "order 1\n",
      "   mwe 1\n",
      "   nmod 2\n",
      "   dobj 3\n",
      "   nsubjpass 1\n",
      "rel 1\n",
      "   compound 1\n",
      "   dobj 2\n",
      "   nsubj 1\n",
      "   conj 1\n",
      "espn 1\n",
      "   nmod:poss 1\n",
      "   conj 1\n",
      "   nsubj 1\n",
      "output 1\n",
      "   nsubj 1\n",
      "process 1\n",
      "   nsubj 1\n",
      "   dobj 2\n",
      "   ROOT 1\n",
      "stop 1\n",
      "   amod 2\n",
      "   compound 3\n",
      "   nmod 1\n",
      "   nsubj 1\n",
      "   conj 1\n",
      "qualiti 1\n",
      "   nsubj 1\n",
      "   nmod 2\n",
      "   conj 3\n",
      "   dobj 3\n",
      "   dep 1\n",
      "   compound 1\n",
      "   amod 1\n",
      "criterion 1\n",
      "   nmod 1\n",
      "   nsubj 1\n",
      "flavor 1\n",
      "   nsubjpass 1\n",
      "   dobj 1\n",
      "robe 1\n",
      "   appos 1\n",
      "   nsubj 1\n",
      "impact 1\n",
      "   nmod 1\n",
      "   dobj 2\n",
      "   nsubj 1\n",
      "   dep 1\n",
      "ocus 1\n",
      "   dobj 3\n",
      "   compound 7\n",
      "   nsubj 1\n",
      "   nmod 2\n",
      "   dep 1\n",
      "   appos 1\n",
      "score 1\n",
      "   nsubj 1\n",
      "   dobj 1\n",
      "   dep 1\n",
      "modul 1\n",
      "   nsubj 1\n",
      "equival 1\n",
      "   root 2\n",
      "   dep 2\n",
      "   nsubj 1\n",
      "   advcl 1\n",
      "   amod 8\n",
      "complement 1\n",
      "   dep 1\n",
      "   nsubj 1\n",
      "   compound 6\n",
      "superset 1\n",
      "   ROOT 1\n",
      "   compound 5\n",
      "   ccomp 1\n",
      "   nsubj 1\n",
      "subset 1\n",
      "   ROOT 3\n",
      "   compound 2\n",
      "   nsubj 1\n",
      "brain 1\n",
      "   dep 1\n",
      "   ROOT 1\n",
      "   dobj 1\n",
      "   nmod 1\n",
      "   nsubj 1\n",
      "sketch 1\n",
      "   nsubjpass 1\n",
      "experiments 1\n",
      "   nsubj 1\n",
      "gain 1\n",
      "   nsubj 1\n",
      "percentag 1\n",
      "   nmod 2\n",
      "   conj 1\n",
      "   ROOT 1\n",
      "   dobj 2\n",
      "   nsubj 1\n",
      "% 1\n",
      "   nmod 2\n",
      "   compound 1\n",
      "   nsubj 1\n",
      "   dobj 1\n",
      "   conj 1\n",
      "   amod 1\n",
      "judgment 1\n",
      "   nsubjpass 1\n",
      "mistak 1\n",
      "   nsubj 1\n",
      "scheme 1\n",
      "   nsubj 1\n",
      "intuit 1\n",
      "   nsubj 1\n",
      "distinct 1\n",
      "   nsubj 1\n",
      "purpos 1\n",
      "   nsubj 1\n",
      "formul 1\n",
      "   nsubj 1\n",
      "rank 0\n",
      "   conj 17\n",
      "   amod 12\n",
      "   ROOT 3\n",
      "   nmod 4\n",
      "   dep 2\n",
      "   compound 1\n",
      "   dobj 1\n",
      "   acl:relcl 1\n",
      "a 0\n",
      "   compound 7\n",
      "   det 209\n",
      "   dobj 1\n",
      "   acl:relcl 1\n",
      "person 0\n",
      "   compound 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   amod 10\n",
      "   conj 1\n",
      "bias 0\n",
      "   amod 25\n",
      "   conj 2\n",
      "   compound 4\n",
      "   dobj 2\n",
      "   ROOT 1\n",
      "   nmod 4\n",
      "   dep 1\n",
      "abstract 0\n",
      "   nmod 1\n",
      "in 0\n",
      "   case 202\n",
      "   mark 10\n",
      "   appos 2\n",
      "   advmod 1\n",
      "   dep 1\n",
      "present 0\n",
      "   acl:relcl 2\n",
      "   ROOT 1\n",
      "   compound 1\n",
      "discoveri 0\n",
      "   dobj 6\n",
      "   compound 15\n",
      "   nmod 7\n",
      "   dep 2\n",
      "   ROOT 2\n",
      "data-intens 0\n",
      "   amod 20\n",
      "support 0\n",
      "   ROOT 5\n",
      "   compound 2\n",
      "   nmod 1\n",
      "   acl:relcl 3\n",
      "   conj 1\n",
      "   ccomp 1\n",
      "   dep 1\n",
      "   dobj 1\n",
      "   advcl 3\n",
      "first 0\n",
      "   amod 16\n",
      "   advmod 4\n",
      "view 0\n",
      "   dobj 5\n",
      "   ccomp 1\n",
      "uniqu 0\n",
      "   amod 2\n",
      "featur 0\n",
      "   dobj 1\n",
      "abl 0\n",
      "   parataxis 1\n",
      "determin 0\n",
      "   xcomp 4\n",
      "   conj 1\n",
      "   advcl 3\n",
      "   root 1\n",
      "   acl 1\n",
      "   ROOT 1\n",
      "few 0\n",
      "   amod 8\n",
      "   advmod 1\n",
      "interact 0\n",
      "   nmod 3\n",
      "target 0\n",
      "   compound 32\n",
      "   dobj 7\n",
      "   xcomp 1\n",
      "   nmod 15\n",
      "   conj 1\n",
      "   appos 1\n",
      "   nmod:poss 1\n",
      "evalu 0\n",
      "   conj 3\n",
      "   acl 1\n",
      "   advcl 3\n",
      "   ROOT 8\n",
      "   xcomp 6\n",
      "   nmod 2\n",
      "respect 0\n",
      "   nmod 11\n",
      "   advmod 1\n",
      "other 0\n",
      "   amod 20\n",
      "   nmod 4\n",
      "use 0\n",
      "   acl:relcl 2\n",
      "   xcomp 12\n",
      "   nmod 2\n",
      "   ROOT 9\n",
      "   acl 14\n",
      "   parataxis 2\n",
      "   conj 1\n",
      "   advcl 3\n",
      "   ccomp 1\n",
      "   dep 2\n",
      "   dobj 1\n",
      "value-ad 0\n",
      "   amod 1\n",
      "metadata 0\n",
      "   nmod 2\n",
      "   dobj 1\n",
      "   compound 1\n",
      "optim 0\n",
      "   compound 1\n",
      "   dobj 2\n",
      "   advcl 1\n",
      "   nmod 1\n",
      "focal 0\n",
      "   amod 47\n",
      "   compound 1\n",
      "   xcomp 1\n",
      "further 0\n",
      "   advmod 4\n",
      "improv 0\n",
      "   xcomp 1\n",
      "   dobj 1\n",
      "   acl 1\n",
      "effect 0\n",
      "   dobj 5\n",
      "   amod 2\n",
      "   advmod 2\n",
      "   conj 2\n",
      "   nmod 1\n",
      "   dep 1\n",
      "   ROOT 1\n",
      "   acl:relcl 1\n",
      "basic 0\n",
      "   amod 7\n",
      "initi 0\n",
      "   amod 6\n",
      "   ROOT 1\n",
      "show 0\n",
      "   acl 2\n",
      "   ROOT 14\n",
      "   parataxis 1\n",
      "   ccomp 1\n",
      "   compound 1\n",
      "   xcomp 1\n",
      "subject 0\n",
      "   compound 1\n",
      "   amod 1\n",
      "descriptor 0\n",
      "   compound 1\n",
      "h. 0\n",
      "   conj 1\n",
      "   compound 2\n",
      "online 0\n",
      "   compound 1\n",
      "informat 0\n",
      "   compound 6\n",
      "   nmod 1\n",
      "web-bas 0\n",
      "   amod 1\n",
      "gener 0\n",
      "   compound 1\n",
      "experiment 0\n",
      "   compound 1\n",
      "   dep 1\n",
      "   nmod 1\n",
      "   amod 1\n",
      "introduction 0\n",
      "   compound 1\n",
      "most 0\n",
      "   amod 4\n",
      "   ROOT 1\n",
      "   advmod 6\n",
      "today 0\n",
      "   nmod:tmod 2\n",
      "web-en 0\n",
      "   amod 1\n",
      "applic 0\n",
      "   dep 1\n",
      "   conj 1\n",
      "access 0\n",
      "   acl:relcl 1\n",
      "   dobj 7\n",
      "messag 0\n",
      "   compound 1\n",
      "typic 0\n",
      "   advmod 6\n",
      "   amod 2\n",
      "standard 0\n",
      "   nmod 1\n",
      "   amod 2\n",
      "such 0\n",
      "   case 2\n",
      "   mark 1\n",
      "   amod 2\n",
      "   xcomp 1\n",
      "   det:predet 1\n",
      "xml 0\n",
      "   nmod 1\n",
      "   compound 4\n",
      "wsdl 0\n",
      "   conj 1\n",
      "soap 0\n",
      "   conj 1\n",
      "mani 0\n",
      "   amod 5\n",
      "   dobj 1\n",
      "   advmod 1\n",
      "mantra 0\n",
      "   nmod 1\n",
      "major 0\n",
      "   amod 1\n",
      "softwar 0\n",
      "   compound 1\n",
      "develop 0\n",
      "   nmod 1\n",
      "   compound 1\n",
      "busi 0\n",
      "   compound 4\n",
      "exchang 0\n",
      "   conj 1\n",
      "ecom-merc 0\n",
      "   compound 1\n",
      "search 0\n",
      "   compound 4\n",
      "   nmod 1\n",
      "   conj 1\n",
      "   acl 1\n",
      "   dep 1\n",
      "engin 0\n",
      "   conj 1\n",
      "   dobj 1\n",
      "   nmod 1\n",
      "   compound 1\n",
      "larg 0\n",
      "   amod 3\n",
      "nsf 0\n",
      "   compound 2\n",
      "cns 0\n",
      "   compound 1\n",
      "ccr 0\n",
      "   nmod 1\n",
      "itr 0\n",
      "   conj 1\n",
      "doe 0\n",
      "   compound 1\n",
      "   aux 3\n",
      "scidac 0\n",
      "   conj 1\n",
      "darpa 0\n",
      "   conj 1\n",
      "cercs 0\n",
      "   compound 1\n",
      "grant 0\n",
      "   conj 3\n",
      "   ROOT 1\n",
      "ibm 0\n",
      "   compound 3\n",
      "   conj 1\n",
      "   dep 1\n",
      "   ROOT 1\n",
      "faculti 0\n",
      "   compound 1\n",
      "award 0\n",
      "   conj 1\n",
      "sur 0\n",
      "   compound 1\n",
      "hp 0\n",
      "   compound 1\n",
      "equipment 0\n",
      "   compound 1\n",
      "llnl 0\n",
      "   compound 1\n",
      "ldrd 0\n",
      "   conj 1\n",
      "digit 0\n",
      "   amod 2\n",
      "hard 0\n",
      "   conj 1\n",
      "part 0\n",
      "   conj 1\n",
      "work 0\n",
      "   nmod 3\n",
      "   compound 2\n",
      "classroom 0\n",
      "   conj 1\n",
      "fee 0\n",
      "   nmod 1\n",
      "   conj 1\n",
      "profit 0\n",
      "   nmod 1\n",
      "commerci 0\n",
      "   amod 2\n",
      "notic 0\n",
      "   dobj 1\n",
      "full 0\n",
      "   amod 1\n",
      "citat 0\n",
      "   conj 1\n",
      "page 0\n",
      "   nmod 1\n",
      "   root 1\n",
      "   ROOT 1\n",
      "server 0\n",
      "   nmod 1\n",
      "   compound 1\n",
      "list 0\n",
      "   nmod 2\n",
      "   ROOT 1\n",
      "   conj 2\n",
      "prior 0\n",
      "   advmod 2\n",
      "   amod 1\n",
      "specif 0\n",
      "   amod 1\n",
      "   advmod 1\n",
      "icsoc 0\n",
      "   ROOT 1\n",
      "04 0\n",
      "   nummod 2\n",
      "   ROOT 1\n",
      "novemb 0\n",
      "   appos 1\n",
      "new 0\n",
      "   compound 2\n",
      "   amod 3\n",
      "   ROOT 1\n",
      "york 0\n",
      "   conj 2\n",
      "usa 0\n",
      "   appos 1\n",
      "acm 0\n",
      "   ROOT 1\n",
      "   compound 2\n",
      "copyright 0\n",
      "   compound 1\n",
      "huge 0\n",
      "   amod 1\n",
      "store 0\n",
      "   nmod 1\n",
      "   dobj 1\n",
      "   acl 1\n",
      "tool 0\n",
      "   conj 2\n",
      "   nmod 1\n",
      "soap-base 0\n",
      "   conj 1\n",
      "interfac 0\n",
      "   dobj 1\n",
      "   nmod 1\n",
      "repositori 0\n",
      "   nmod 6\n",
      "advanc 0\n",
      "   amod 1\n",
      "oper 0\n",
      "   nmod 1\n",
      "billion 0\n",
      "   root 1\n",
      "item 0\n",
      "   nmod 1\n",
      "life 0\n",
      "   compound 2\n",
      "scienc 0\n",
      "   compound 2\n",
      "   nmod 1\n",
      "domain 0\n",
      "   nmod 1\n",
      "bioinformat 0\n",
      "   compound 3\n",
      "human-in-the-loop 0\n",
      "   amod 1\n",
      "direct 0\n",
      "   amod 1\n",
      "   advmod 1\n",
      "   conj 1\n",
      "unpreced 0\n",
      "   amod 1\n",
      "amount 0\n",
      "   nmod 1\n",
      "raw 0\n",
      "   amod 2\n",
      "special 0\n",
      "   amod 2\n",
      "   ROOT 2\n",
      "high-level 0\n",
      "   amod 1\n",
      "visibl 0\n",
      "   nmod 1\n",
      "service-ori 0\n",
      "   compound 1\n",
      "   amod 1\n",
      "comput 0\n",
      "   compound 1\n",
      "   parataxis 1\n",
      "   advmod 1\n",
      "   ROOT 3\n",
      "paradigm 0\n",
      "   conj 1\n",
      "effici 0\n",
      "   amod 8\n",
      "   conj 1\n",
      "   dobj 4\n",
      "   nmod 1\n",
      "   root 1\n",
      "critic 0\n",
      "   ROOT 1\n",
      "   amod 4\n",
      "   nmod 1\n",
      "tremend 0\n",
      "   amod 1\n",
      "opportun 0\n",
      "   nmod 1\n",
      "offer 0\n",
      "   acl 4\n",
      "   nmod 1\n",
      "   ROOT 2\n",
      "   conj 1\n",
      "   acl:relcl 2\n",
      "collabor 0\n",
      "   nmod 1\n",
      "composit 0\n",
      "   conj 1\n",
      "   ROOT 1\n",
      "potenti 0\n",
      "   amod 1\n",
      "   advmod 1\n",
      "partner 0\n",
      "   dobj 1\n",
      "competitor 0\n",
      "   dobj 1\n",
      "competit 0\n",
      "   amod 1\n",
      "edg 0\n",
      "   dobj 1\n",
      "classifi 0\n",
      "   ROOT 2\n",
      "   nmod 1\n",
      "   advcl 1\n",
      "   compound 1\n",
      "   conj 1\n",
      "current 0\n",
      "   amod 6\n",
      "categorization-bas 0\n",
      "   amod 1\n",
      "   nmod 1\n",
      "relevance-bas 0\n",
      "   amod 1\n",
      "certain 0\n",
      "   amod 4\n",
      "common 0\n",
      "   amod 6\n",
      "   nmod 2\n",
      "   dobj 1\n",
      "properti 0\n",
      "   nmod 1\n",
      "uddi 0\n",
      "   nmod 1\n",
      "   compound 2\n",
      "   ROOT 1\n",
      "registry-bas 0\n",
      "   amod 1\n",
      "like 0\n",
      "   case 5\n",
      "   mark 1\n",
      "   dep 1\n",
      "   acl 1\n",
      "microsoft 0\n",
      "   nmod 1\n",
      "   ROOT 2\n",
      "   compound 1\n",
      "blast 0\n",
      "   compound 9\n",
      "   dep 1\n",
      "   nmod 2\n",
      "   ROOT 1\n",
      "capabl 0\n",
      "   dobj 1\n",
      "   conj 1\n",
      "   ROOT 1\n",
      "on-lin 0\n",
      "   amod 1\n",
      "auction 0\n",
      "   dobj 1\n",
      "second 0\n",
      "   amod 7\n",
      "   ROOT 1\n",
      "reason 0\n",
      "   dobj 1\n",
      "   advmod 3\n",
      "   conj 1\n",
      "same 0\n",
      "   amod 5\n",
      "   xcomp 1\n",
      "   ROOT 1\n",
      "content 0\n",
      "   nmod 1\n",
      "   conj 1\n",
      "   dobj 1\n",
      "   compound 1\n",
      "ncbi 0\n",
      "   nmod 3\n",
      "   nmod:poss 4\n",
      "   compound 5\n",
      "top-ten 0\n",
      "   amod 2\n",
      "more 0\n",
      "   amod 9\n",
      "   advmod 12\n",
      "   conj 1\n",
      "coverag 0\n",
      "   dobj 5\n",
      "   nmod 4\n",
      "   conj 2\n",
      "complementari 0\n",
      "   amod 3\n",
      "   ROOT 3\n",
      "   conj 1\n",
      "Â· 0\n",
      "   compound 15\n",
      "   nummod 5\n",
      "   appos 2\n",
      "   advmod 3\n",
      "   dep 3\n",
      "dna 0\n",
      "   dobj 1\n",
      "protein 0\n",
      "   compound 1\n",
      "   nmod 1\n",
      "   appos 1\n",
      "   dep 2\n",
      "   conj 1\n",
      "sequenc 0\n",
      "   compound 2\n",
      "   nmod 1\n",
      "librari 0\n",
      "   conj 1\n",
      "   dep 1\n",
      "   compound 1\n",
      "pointer 0\n",
      "   dobj 1\n",
      "relat 0\n",
      "   ccomp 1\n",
      "   xcomp 1\n",
      "   amod 13\n",
      "   advmod 3\n",
      "   ROOT 1\n",
      "   dep 1\n",
      "best 0\n",
      "   advmod 1\n",
      "   amod 4\n",
      "health 0\n",
      "   compound 1\n",
      "   dep 1\n",
      "familiar 0\n",
      "   amod 1\n",
      "pubm 0\n",
      "   compound 6\n",
      "   nmod 14\n",
      "   conj 2\n",
      "   xcomp 2\n",
      "   amod 1\n",
      "   dobj 1\n",
      "med-153 0\n",
      "   compound 1\n",
      "ical 0\n",
      "   amod 1\n",
      "literatur 0\n",
      "   compound 3\n",
      "   nmod 1\n",
      "medic 0\n",
      "   amod 5\n",
      "knowledg 0\n",
      "   nmod 2\n",
      "   conj 1\n",
      "convent 0\n",
      "   amod 1\n",
      "service-registry-bas 0\n",
      "   amod 1\n",
      "pubmed-rel 0\n",
      "   amod 1\n",
      "general 0\n",
      "   amod 6\n",
      "   nmod 3\n",
      "   ccomp 1\n",
      "   xcomp 1\n",
      "fundament 0\n",
      "   amod 2\n",
      "personalization-bas 0\n",
      "   amod 1\n",
      "descript 0\n",
      "   nmod 3\n",
      "   compound 1\n",
      "insuffici 0\n",
      "   parataxis 1\n",
      "inadequ 0\n",
      "   conj 1\n",
      "   advcl 1\n",
      "particular 0\n",
      "   amod 6\n",
      "   advmod 1\n",
      "scope 0\n",
      "   dobj 3\n",
      "   conj 2\n",
      "previous 0\n",
      "   advmod 3\n",
      "   amod 3\n",
      "propos 0\n",
      "   root 1\n",
      "   ROOT 2\n",
      "fall 0\n",
      "   nmod 1\n",
      "no 0\n",
      "   neg 11\n",
      "   compound 3\n",
      "signific 0\n",
      "   amod 5\n",
      "   advmod 3\n",
      "human 0\n",
      "   amod 1\n",
      "intervent 0\n",
      "   nmod 1\n",
      "detect 0\n",
      "   conj 1\n",
      "   ROOT 1\n",
      "degre 0\n",
      "   conj 1\n",
      "   dobj 3\n",
      "leverag 0\n",
      "   dep 2\n",
      "   compound 1\n",
      "   xcomp 1\n",
      "inform 0\n",
      "   dobj 3\n",
      "   nmod 2\n",
      "   compound 2\n",
      "generat 0\n",
      "   advcl 5\n",
      "   acl 8\n",
      "   ROOT 2\n",
      "   xcomp 4\n",
      "   acl:relcl 1\n",
      "   ccomp 2\n",
      "   dep 1\n",
      "   compound 1\n",
      "seri 0\n",
      "   dobj 3\n",
      "us 0\n",
      "   dobj 5\n",
      "   appos 1\n",
      "high 0\n",
      "   advmod 1\n",
      "   amod 8\n",
      "   dep 2\n",
      "   advcl 1\n",
      "simul 0\n",
      "   nmod 1\n",
      "statement 0\n",
      "   dep 1\n",
      "discours 0\n",
      "   compound 4\n",
      "= 0\n",
      "   parataxis 5\n",
      "   amod 21\n",
      "   ccomp 4\n",
      "   dep 13\n",
      "   xcomp 4\n",
      "   nmod 1\n",
      "   advcl 1\n",
      "   ROOT 4\n",
      "   root 1\n",
      "   acl:relcl 1\n",
      "   conj 3\n",
      "s 0\n",
      "   dep 13\n",
      "   conj 3\n",
      "   compound 63\n",
      "   dobj 6\n",
      "   appos 29\n",
      "   nmod 13\n",
      "respons 0\n",
      "   nmod 2\n",
      "   conj 1\n",
      "doc 0\n",
      "   dep 7\n",
      "   appos 2\n",
      "   compound 9\n",
      "correspond 0\n",
      "   acl 2\n",
      "   amod 4\n",
      "genet 0\n",
      "   amod 1\n",
      "journal 0\n",
      "   compound 1\n",
      "   ROOT 2\n",
      "   dobj 1\n",
      "articl 0\n",
      "   nmod 6\n",
      "   dobj 3\n",
      "t 0\n",
      "   dep 7\n",
      "   appos 5\n",
      "   compound 26\n",
      "   nmod 1\n",
      "   conj 1\n",
      "tag 0\n",
      "   nmod 1\n",
      "refin 0\n",
      "   ROOT 2\n",
      "   conj 1\n",
      "   acl:relcl 1\n",
      "   dobj 1\n",
      "   dep 1\n",
      "prefix 0\n",
      "   dobj 2\n",
      "   nmod 1\n",
      "suffix 0\n",
      "   conj 1\n",
      "   nmod 1\n",
      "vector-spac 0\n",
      "   amod 1\n",
      "vector 0\n",
      "   nmod 10\n",
      "   compound 4\n",
      "   ROOT 2\n",
      "   dobj 2\n",
      "non-zero 0\n",
      "   amod 6\n",
      "singl 0\n",
      "   amod 11\n",
      "aggreg 0\n",
      "   compound 3\n",
      "overal 0\n",
      "   amod 6\n",
      "   advmod 1\n",
      "bag-of-word 0\n",
      "   amod 1\n",
      "   compound 1\n",
      "indiffer 0\n",
      "   acl:relcl 1\n",
      "structur 0\n",
      "   nmod 1\n",
      "   amod 2\n",
      "inher 0\n",
      "   amod 3\n",
      "power 0\n",
      "   ROOT 1\n",
      "   dobj 2\n",
      "   compound 1\n",
      "section 0\n",
      "   nmod 12\n",
      "burden 0\n",
      "   nmod 1\n",
      "   dobj 1\n",
      "comparison 0\n",
      "   nmod 1\n",
      "   root 1\n",
      "futur 0\n",
      "   amod 3\n",
      "version 0\n",
      "   dobj 2\n",
      "schema 0\n",
      "   compound 1\n",
      "match 0\n",
      "   dobj 1\n",
      "   ccomp 1\n",
      "   amod 2\n",
      "   acl 1\n",
      "ontolog 0\n",
      "   dep 1\n",
      "repres 0\n",
      "   ROOT 1\n",
      "   amod 3\n",
      "   acl 1\n",
      "frequenc 0\n",
      "   conj 2\n",
      "   acl:relcl 1\n",
      "   nmod 3\n",
      "   xcomp 2\n",
      "   dobj 1\n",
      "   advcl 1\n",
      "req 0\n",
      "   compound 6\n",
      "   dep 1\n",
      "   appos 1\n",
      "   nmod 1\n",
      "   advcl 1\n",
      "jn 0\n",
      "   appos 1\n",
      "occurr 0\n",
      "   nmod 1\n",
      "   compound 1\n",
      "occurrence-bas 0\n",
      "   amod 1\n",
      "term-frequ 0\n",
      "   compound 1\n",
      "invers 0\n",
      "   amod 1\n",
      "document-frequ 0\n",
      "   compound 1\n",
      "tfidf 0\n",
      "   appos 1\n",
      "   compound 2\n",
      "characterist 0\n",
      "   nmod 1\n",
      "   conj 1\n",
      "encod 0\n",
      "   nmod 1\n",
      "servfreq 0\n",
      "   conj 1\n",
      "servf 0\n",
      "   compound 5\n",
      "count 0\n",
      "   compound 1\n",
      "   nmod 2\n",
      "   advcl 1\n",
      "   ROOT 1\n",
      "   conj 1\n",
      "doccount 0\n",
      "   conj 1\n",
      "   appos 1\n",
      "technic 0\n",
      "   amod 2\n",
      "   compound 1\n",
      "estimat 0\n",
      "   compound 1\n",
      "complet 0\n",
      "   amod 1\n",
      "   advmod 1\n",
      "actual 0\n",
      "   amod 4\n",
      "   advmod 1\n",
      "asummari 0\n",
      "   conj 1\n",
      "   nmod 1\n",
      "   dep 1\n",
      "enorm 0\n",
      "   amod 1\n",
      "non-trivi 0\n",
      "   amod 1\n",
      "cost 0\n",
      "   nmod 1\n",
      "   conj 1\n",
      "individu 0\n",
      "   amod 1\n",
      "transfer 0\n",
      "   conj 1\n",
      "unreason 0\n",
      "   dep 1\n",
      "avail 0\n",
      "   amod 1\n",
      "context 0\n",
      "   nmod 3\n",
      "databas 0\n",
      "   nmod 5\n",
      "   dobj 2\n",
      "   compound 4\n",
      "sever 0\n",
      "   amod 11\n",
      "   advmod 1\n",
      "small 0\n",
      "   amod 3\n",
      "   xcomp 1\n",
      "document-bas 0\n",
      "   amod 2\n",
      "estim 0\n",
      "   amod 11\n",
      "   acl 1\n",
      "   advcl 1\n",
      "   dobj 4\n",
      "   compound 2\n",
      "   nmod 1\n",
      "total 0\n",
      "   amod 7\n",
      "   nmod 1\n",
      "close 0\n",
      "   advmod 1\n",
      "   amod 3\n",
      "aim 0\n",
      "   dobj 1\n",
      "random 0\n",
      "   amod 11\n",
      "   compound 4\n",
      "   advmod 10\n",
      "   nmod 1\n",
      "query-bas 0\n",
      "   amod 5\n",
      "select 0\n",
      "   ROOT 9\n",
      "   compound 11\n",
      "   xcomp 2\n",
      "   nmod 6\n",
      "   conj 3\n",
      "   acl 5\n",
      "   advcl 5\n",
      "   amod 2\n",
      "   ccomp 1\n",
      "   acl:relcl 7\n",
      "   appos 1\n",
      "   dobj 1\n",
      "o 0\n",
      "   compound 1\n",
      "unfett 0\n",
      "   amod 1\n",
      "unbias 0\n",
      "   amod 10\n",
      "   conj 2\n",
      "equal 0\n",
      "   advmod 1\n",
      "   amod 1\n",
      "   dobj 1\n",
      "unrealist 0\n",
      "   ROOT 1\n",
      "practic 0\n",
      "   nmod 2\n",
      "request-respons 0\n",
      "   amod 1\n",
      "good 0\n",
      "   amod 2\n",
      "approxim 0\n",
      "   nmod 1\n",
      "   advcl 1\n",
      "   advmod 1\n",
      "   amod 2\n",
      "accur 0\n",
      "   amod 2\n",
      "repeat- 0\n",
      "   amod 1\n",
      "limit 0\n",
      "   amod 2\n",
      "   nmod 2\n",
      "return 0\n",
      "   amod 1\n",
      "   compound 2\n",
      "   ROOT 1\n",
      "   acl 1\n",
      "   advcl 1\n",
      "incomplet 0\n",
      "   amod 1\n",
      "entir 0\n",
      "   amod 2\n",
      "dictionari 0\n",
      "   nmod 5\n",
      "rest 0\n",
      "   nmod 1\n",
      "query-bias 0\n",
      "   amod 8\n",
      "appropri 0\n",
      "   amod 5\n",
      "possibl 0\n",
      "   amod 4\n",
      "fair 0\n",
      "   advmod 1\n",
      "   amod 1\n",
      "simpl 0\n",
      "   amod 7\n",
      "   advcl 1\n",
      "straightforward 0\n",
      "   conj 1\n",
      "| 0\n",
      "   nummod 15\n",
      "   dep 1\n",
      "   compound 6\n",
      "max 0\n",
      "   dep 1\n",
      "indic 0\n",
      "   acl 3\n",
      "   advcl 7\n",
      "   conj 1\n",
      "   ROOT 1\n",
      "   xcomp 1\n",
      "   root 1\n",
      "   dep 2\n",
      "highest 0\n",
      "   amod 2\n",
      "lowest 0\n",
      "   amod 1\n",
      "p 0\n",
      "   compound 6\n",
      "   dep 1\n",
      "ubm 0\n",
      "   appos 2\n",
      "   amod 1\n",
      "   nmod 1\n",
      "esp 0\n",
      "   compound 1\n",
      "case 0\n",
      "   nmod 7\n",
      "irrelev 0\n",
      "   xcomp 1\n",
      "   ccomp 1\n",
      "consider 0\n",
      "   amod 1\n",
      "   advmod 1\n",
      "health-rel 0\n",
      "   amod 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global 0\n",
      "   dep 1\n",
      "   amod 1\n",
      "   conj 1\n",
      "valuabl 0\n",
      "   amod 1\n",
      "issu 0\n",
      "   dobj 1\n",
      "mind 0\n",
      "   nmod 1\n",
      "perspect 0\n",
      "   nmod 2\n",
      "   dobj 1\n",
      "step 0\n",
      "   nmod 1\n",
      "help 0\n",
      "   ROOT 2\n",
      "import 0\n",
      "   ROOT 2\n",
      "   ccomp 1\n",
      "inequ 0\n",
      "   compound 1\n",
      "hold 0\n",
      "   acl:relcl 1\n",
      "   dep 2\n",
      "candid 0\n",
      "   compound 2\n",
      "retriev 0\n",
      "   parataxis 1\n",
      "   ROOT 1\n",
      "   xcomp 1\n",
      "   conj 1\n",
      "   nmod 3\n",
      "top 0\n",
      "   amod 2\n",
      "updat 0\n",
      "   compound 1\n",
      "keyword-bas 0\n",
      "   amod 1\n",
      "uddi-directory-bas 0\n",
      "   amod 1\n",
      "sourcebiasedprob 0\n",
      "   dobj 1\n",
      "one-term 0\n",
      "   amod 1\n",
      "q 0\n",
      "   dobj 2\n",
      "top-m 0\n",
      "   amod 2\n",
      "update 0\n",
      "   compound 1\n",
      "meta-data 0\n",
      "   amod 1\n",
      "lower 0\n",
      "   ROOT 1\n",
      "   amod 1\n",
      "   xcomp 1\n",
      "due 0\n",
      "   case 2\n",
      "   advmod 1\n",
      "lack 0\n",
      "   nmod 1\n",
      "rich 0\n",
      "   nmod 1\n",
      "breviti 0\n",
      "   nmod 1\n",
      "simplist 0\n",
      "   amod 1\n",
      "world 0\n",
      "   dobj 1\n",
      "realiti 0\n",
      "   nmod 1\n",
      "magnitud 0\n",
      "   nmod 1\n",
      "arthriti 0\n",
      "   dep 4\n",
      "bacteria 0\n",
      "   appos 2\n",
      "   conj 1\n",
      "cancer 0\n",
      "   appos 2\n",
      "   conj 1\n",
      "simplic 0\n",
      "   nmod 2\n",
      "anim 0\n",
      "   conj 1\n",
      "car 0\n",
      "   conj 1\n",
      "dog 0\n",
      "   conj 1\n",
      "eleph 0\n",
      "   conj 1\n",
      "frog 0\n",
      "   conj 1\n",
      "   appos 1\n",
      "garag 0\n",
      "   conj 1\n",
      "helmet 0\n",
      "   conj 1\n",
      "   appos 1\n",
      "indycar 0\n",
      "   appos 1\n",
      "essenc 0\n",
      "   dobj 1\n",
      "   nmod 1\n",
      "factor 0\n",
      "   nmod 1\n",
      "source-specif 0\n",
      "   amod 1\n",
      "statist 0\n",
      "   nmod 2\n",
      "uniform 0\n",
      "   amod 1\n",
      "   compound 1\n",
      "top-weight 0\n",
      "   amod 1\n",
      "simplest 0\n",
      "   nmod 1\n",
      "probabl 0\n",
      "   dobj 1\n",
      "rob 0\n",
      "   parataxis 1\n",
      "weight-bas 0\n",
      "   amod 2\n",
      "fix 0\n",
      "   amod 1\n",
      "axp 0\n",
      "   compound 2\n",
      "agnost 0\n",
      "   ROOT 1\n",
      "contrast 0\n",
      "   nmod 7\n",
      "axdoc 0\n",
      "   appos 1\n",
      "   dobj 1\n",
      "   nmod 1\n",
      "threshold 0\n",
      "   ROOT 1\n",
      "   compound 3\n",
      "third 0\n",
      "   amod 6\n",
      "greater 0\n",
      "   advcl 1\n",
      "steady-st 0\n",
      "   ROOT 1\n",
      "   dobj 1\n",
      "final 0\n",
      "   amod 2\n",
      "   advmod 1\n",
      "old 0\n",
      "   amod 2\n",
      "stabl 0\n",
      "   xcomp 1\n",
      "space 0\n",
      "   compound 3\n",
      "   dobj 1\n",
      "reader 0\n",
      "   dobj 1\n",
      "report 0\n",
      "   nmod 1\n",
      "   acl:relcl 1\n",
      "   ROOT 1\n",
      "detail 0\n",
      "   amod 1\n",
      "paramet 0\n",
      "   nmod 3\n",
      "   compound 1\n",
      "necessari 0\n",
      "   amod 1\n",
      "topic 0\n",
      "   amod 1\n",
      "   nmod 4\n",
      "   compound 8\n",
      "   advmod 1\n",
      "rang 0\n",
      "   ROOT 2\n",
      "   xcomp 1\n",
      "   advcl 1\n",
      "higher 0\n",
      "   amod 8\n",
      "symmetr 0\n",
      "   amod 1\n",
      "well-known 0\n",
      "   amod 1\n",
      "inner 0\n",
      "   amod 1\n",
      "product 0\n",
      "   dobj 1\n",
      "cosine-bas 0\n",
      "   amod 2\n",
      "orthogon 0\n",
      "   amod 1\n",
      "dissimilar 0\n",
      "   ccomp 1\n",
      "   conj 3\n",
      "   compound 1\n",
      "angl 0\n",
      "   dobj 1\n",
      "length 0\n",
      "   nmod 1\n",
      "tar-156 0\n",
      "   nmod 1\n",
      "multipl 0\n",
      "   amod 3\n",
      "source-target 0\n",
      "   compound 2\n",
      "   amod 5\n",
      "pair 0\n",
      "   nmod 10\n",
      "   compound 2\n",
      "   dobj 1\n",
      "   conj 1\n",
      "identif 0\n",
      "   nmod 2\n",
      "   dobj 1\n",
      "fewer 0\n",
      "   amod 3\n",
      "   dep 1\n",
      "   advmod 1\n",
      "place 0\n",
      "   advcl 1\n",
      "   nmod 1\n",
      "len 0\n",
      "   dobj 1\n",
      "flexibl 0\n",
      "   amod 3\n",
      "build 0\n",
      "   compound 1\n",
      "block 0\n",
      "   dobj 1\n",
      "asymmetr 0\n",
      "   amod 2\n",
      "annot 0\n",
      "   acl:relcl 1\n",
      "   ROOT 1\n",
      "   nmod 1\n",
      "similarity-bas 0\n",
      "   ROOT 1\n",
      "   amod 4\n",
      "hierarchical-bas 0\n",
      "   conj 1\n",
      "   amod 1\n",
      "low 0\n",
      "   conj 1\n",
      "   dep 2\n",
      "   ROOT 1\n",
      "   amod 4\n",
      "< 0\n",
      "   advmod 4\n",
      "   amod 2\n",
      "   dep 1\n",
      "> 0\n",
      "   advmod 5\n",
      "   compound 2\n",
      "   dep 3\n",
      "absolut 0\n",
      "   ccomp 1\n",
      "formal 0\n",
      "   ROOT 1\n",
      "   advmod 5\n",
      "   nmod 1\n",
      "hierarch 0\n",
      "   amod 4\n",
      "addit 0\n",
      "   nmod 6\n",
      "   amod 1\n",
      "text 0\n",
      "   compound 6\n",
      "strict 0\n",
      "   amod 2\n",
      "correct 0\n",
      "   amod 1\n",
      "assign 0\n",
      "   nmod 1\n",
      "   parataxis 1\n",
      "   ROOT 1\n",
      "   amod 1\n",
      "   acl 1\n",
      "hierarchy-bas 0\n",
      "   amod 1\n",
      "inter-servic 0\n",
      "   amod 3\n",
      "foundat 0\n",
      "   dobj 1\n",
      "basi 0\n",
      "   dobj 2\n",
      "answer 0\n",
      "   acl 2\n",
      "   dobj 1\n",
      "form 0\n",
      "   nmod 3\n",
      "x 0\n",
      "   ccomp 1\n",
      "   nmod 4\n",
      "topically-distinct 0\n",
      "   amod 1\n",
      "member 0\n",
      "   dep 1\n",
      "   nmod 1\n",
      "hierarchi 0\n",
      "   nmod 2\n",
      "   dep 1\n",
      "   compound 1\n",
      "choic 0\n",
      "   ROOT 1\n",
      "success 0\n",
      "   nmod 1\n",
      "   dobj 1\n",
      "   compound 1\n",
      "stop-prob 0\n",
      "   amod 1\n",
      "segment 0\n",
      "   ccomp 1\n",
      "   compound 1\n",
      "co-occur 0\n",
      "   amod 1\n",
      "   dep 1\n",
      "   acl 1\n",
      "   xcomp 2\n",
      "   nmod 1\n",
      "main 0\n",
      "   amod 3\n",
      "conjunct 0\n",
      "   nmod 1\n",
      "tabl 0\n",
      "   dep 3\n",
      "   nmod 4\n",
      "   compound 2\n",
      "   ROOT 1\n",
      "care 0\n",
      "   nmod 1\n",
      "educ 0\n",
      "   conj 1\n",
      "famili 0\n",
      "   conj 1\n",
      "manag 0\n",
      "   conj 2\n",
      "gene 0\n",
      "   appos 2\n",
      "   dobj 1\n",
      "nucleotid 0\n",
      "   appos 1\n",
      "   conj 2\n",
      "clinic 0\n",
      "   dep 1\n",
      "   ROOT 2\n",
      "   nmod 1\n",
      "   compound 1\n",
      "noteworthi 0\n",
      "   conj 1\n",
      "taxonomi 0\n",
      "   conj 1\n",
      "molecular 0\n",
      "   conj 1\n",
      "therapi 0\n",
      "   conj 1\n",
      "aid 0\n",
      "   dep 1\n",
      "evid 0\n",
      "   conj 1\n",
      "winter 0\n",
      "   conj 1\n",
      "partit 0\n",
      "   nmod 2\n",
      "   dobj 1\n",
      "disjoint 0\n",
      "   compound 1\n",
      "coars 0\n",
      "   xcomp 1\n",
      "   amod 1\n",
      "word 0\n",
      "   nmod 1\n",
      "frequency-bas 0\n",
      "   amod 1\n",
      "naiv 0\n",
      "   amod 1\n",
      "co-occurr 0\n",
      "   compound 2\n",
      "query-expans 0\n",
      "   nmod 2\n",
      "semant 0\n",
      "   amod 3\n",
      "   nmod 1\n",
      "associ 0\n",
      "   dobj 1\n",
      "popular 0\n",
      "   amod 1\n",
      "jm 0\n",
      "   appos 3\n",
      "clus-focalterm 0\n",
      "   nmod 1\n",
      "num 0\n",
      "   compound 1\n",
      "input 0\n",
      "   compound 2\n",
      "element 0\n",
      "   nmod 1\n",
      "Âµ 0\n",
      "   compound 2\n",
      "   dep 1\n",
      "foreach 0\n",
      "   compound 3\n",
      "   ccomp 1\n",
      "   amod 1\n",
      "sim 0\n",
      "   compound 1\n",
      "   dep 1\n",
      "mu 0\n",
      "   compound 2\n",
      "smallest 0\n",
      "   advcl 1\n",
      "   amod 1\n",
      "nearest 0\n",
      "   amod 1\n",
      "cw 0\n",
      "   compound 3\n",
      "ij 0\n",
      "   compound 1\n",
      "im 0\n",
      "   appos 1\n",
      "chang 0\n",
      "   ccomp 1\n",
      "   nmod 1\n",
      "tere 0\n",
      "   compound 1\n",
      "k-mean 0\n",
      "   dobj 1\n",
      "   compound 1\n",
      "least 0\n",
      "   dep 1\n",
      "distant 0\n",
      "   dep 1\n",
      "close- 0\n",
      "   advcl 1\n",
      "cycl 0\n",
      "   ROOT 1\n",
      "   compound 1\n",
      "   dobj 1\n",
      "   nmod 2\n",
      "jc 0\n",
      "   compound 3\n",
      "formula 0\n",
      "   compound 1\n",
      "averag 0\n",
      "   amod 9\n",
      "term-vector 0\n",
      "   nmod 1\n",
      "focal-term 0\n",
      "   amod 1\n",
      "focal-bas 0\n",
      "   amod 1\n",
      "round-robin 0\n",
      "   compound 1\n",
      "turn 0\n",
      "   nmod 1\n",
      "strategi 0\n",
      "   nmod 1\n",
      "criteria 0\n",
      "   nmod 1\n",
      "benefit 0\n",
      "   dobj 1\n",
      "fourth 0\n",
      "   amod 1\n",
      "efficaci 0\n",
      "   dobj 1\n",
      "percent 0\n",
      "   compound 1\n",
      "newsgroup 0\n",
      "   nmod 11\n",
      "   compound 11\n",
      "   dobj 3\n",
      "   conj 2\n",
      "   dep 1\n",
      "divers 0\n",
      "   dobj 1\n",
      "real-world 0\n",
      "   amod 3\n",
      "modest 0\n",
      "   amod 1\n",
      "control 0\n",
      "   advcl 1\n",
      "effort 0\n",
      "   conj 1\n",
      "   nmod 2\n",
      "overload 0\n",
      "   acl 1\n",
      "dataset 0\n",
      "   nmod 2\n",
      "rigor 0\n",
      "   amod 1\n",
      "valid 0\n",
      "   nmod 1\n",
      "   advcl 1\n",
      "usenet 0\n",
      "   compound 2\n",
      "period 0\n",
      "   nmod 1\n",
      "june 0\n",
      "   dep 1\n",
      "juli 0\n",
      "   nmod 1\n",
      "binari 0\n",
      "   amod 1\n",
      "heterogen 0\n",
      "   dobj 1\n",
      "   nmod 1\n",
      "mix 0\n",
      "   amod 5\n",
      "subgroup 0\n",
      "   nmod 1\n",
      "comp.unix 0\n",
      "   nmod 1\n",
      "gb 0\n",
      "   compound 1\n",
      "worth 0\n",
      "   nmod 1\n",
      "profus 0\n",
      "   nmod 1\n",
      "   ROOT 1\n",
      "directori 0\n",
      "   dep 2\n",
      "   dobj 2\n",
      "   compound 1\n",
      "single-word 0\n",
      "   amod 1\n",
      "unix 0\n",
      "   compound 3\n",
      "maximum 0\n",
      "   dobj 2\n",
      "java 0\n",
      "   nmod 1\n",
      "efficienc 0\n",
      "   advcl 1\n",
      "   compound 1\n",
      "collection-specif 0\n",
      "   amod 1\n",
      "english 0\n",
      "   compound 1\n",
      "subsequ 0\n",
      "   amod 1\n",
      "clear 0\n",
      "   advmod 1\n",
      "   amod 1\n",
      "perl.misc 0\n",
      "   conj 1\n",
      "breakdown 0\n",
      "   dobj 1\n",
      "level 0\n",
      "   dobj 1\n",
      "time 0\n",
      "   nummod 1\n",
      "source-relev 0\n",
      "   amod 2\n",
      "   parataxis 1\n",
      "two-tim 0\n",
      "   xcomp 1\n",
      "better 0\n",
      "   advmod 1\n",
      "unabl 0\n",
      "   ROOT 1\n",
      "ama 0\n",
      "   ROOT 2\n",
      "   dep 1\n",
      "open 0\n",
      "   ROOT 1\n",
      "   dep 1\n",
      "   compound 1\n",
      "webmd 0\n",
      "   ROOT 2\n",
      "   dep 1\n",
      "linux 0\n",
      "   compound 2\n",
      "healthatoz 0\n",
      "   ROOT 2\n",
      "   nmod 1\n",
      "   dep 1\n",
      "devguru 0\n",
      "   ROOT 1\n",
      "   appos 1\n",
      "magazin 0\n",
      "   ROOT 2\n",
      "   conj 1\n",
      "familytre 0\n",
      "   compound 1\n",
      "   appos 1\n",
      "mayo 0\n",
      "   compound 4\n",
      "monster 0\n",
      "   ROOT 1\n",
      "   compound 2\n",
      "novel 0\n",
      "   compound 1\n",
      "   amod 1\n",
      "hous 0\n",
      "   ROOT 2\n",
      "januari 0\n",
      "   compound 1\n",
      "bbc 0\n",
      "   compound 1\n",
      "comp.sys.mac.system 0\n",
      "   compound 3\n",
      "   nmod 1\n",
      "   acl 1\n",
      "comp.unix.misc 0\n",
      "   compound 1\n",
      "   amod 1\n",
      "gnu.emacs.help 0\n",
      "   dep 1\n",
      "rec.games.chess.misc 0\n",
      "   compound 1\n",
      "   dobj 1\n",
      "rec.org.sca 0\n",
      "   compound 1\n",
      "rec.pets.cats.misc 0\n",
      "   compound 1\n",
      "sci.physics.research 0\n",
      "   compound 1\n",
      "soc.culture.hawaii 0\n",
      "   compound 1\n",
      "talk.religion.misc 0\n",
      "   compound 1\n",
      "top-10 0\n",
      "   amod 2\n",
      "column 0\n",
      "   nmod 1\n",
      "parenthesi 0\n",
      "   nmod 1\n",
      "genealog 0\n",
      "   amod 1\n",
      "top-eight 0\n",
      "   amod 1\n",
      "scientif 0\n",
      "   amod 1\n",
      "bioinformatics-rel 0\n",
      "   conj 1\n",
      "job 0\n",
      "   compound 2\n",
      "   dep 1\n",
      "next 0\n",
      "   advmod 1\n",
      "   amod 2\n",
      "consensus 0\n",
      "   compound 1\n",
      "opinion 0\n",
      "   nmod 1\n",
      "volunt 0\n",
      "   nmod 1\n",
      "lone 0\n",
      "   amod 1\n",
      "failur 0\n",
      "   nmod 1\n",
      "attribut 0\n",
      "   ccomp 1\n",
      "error 0\n",
      "   nmod 1\n",
      "doubl 0\n",
      "   ROOT 1\n",
      "class 0\n",
      "   dobj 1\n",
      "sci.physics.particl 0\n",
      "   dobj 1\n",
      "   compound 1\n",
      "   amod 1\n",
      "mixed11 0\n",
      "   nmod 1\n",
      "physics-rel 0\n",
      "   amod 1\n",
      "backgammon 0\n",
      "   nmod 1\n",
      "juggl 0\n",
      "   conj 1\n",
      "telecommun 0\n",
      "   conj 1\n",
      "field 0\n",
      "   nmod 1\n",
      "cricket 0\n",
      "   conj 1\n",
      "unrel 0\n",
      "   amod 2\n",
      "misc.immigration.usa 0\n",
      "   nmod 1\n",
      "   dep 1\n",
      "rate 0\n",
      "   nmod 1\n",
      "littl 0\n",
      "   amod 1\n",
      "single-top 0\n",
      "   amod 1\n",
      "origin 0\n",
      "   amod 1\n",
      "minor 0\n",
      "   amod 1\n",
      "circumst 0\n",
      "   nmod 1\n",
      "related 0\n",
      "   amod 1\n",
      "aspect 0\n",
      "   dobj 1\n",
      "feder 0\n",
      "   amod 2\n",
      "environ 0\n",
      "   nmod 4\n",
      "quality-of-servic 0\n",
      "   amod 1\n",
      "guarante 0\n",
      "   dobj 1\n",
      "reput 0\n",
      "   nmod 1\n",
      "re-peat 0\n",
      "   amod 1\n",
      "unknown 0\n",
      "   compound 1\n",
      "intern 0\n",
      "   nmod 1\n",
      "feedback 0\n",
      "   compound 1\n",
      "gravano 0\n",
      "   compound 1\n",
      "   appos 2\n",
      "extens 0\n",
      "   dobj 1\n",
      "callan-styl 0\n",
      "   amod 1\n",
      "classif 0\n",
      "   nmod 2\n",
      "pre-determin 0\n",
      "   amod 1\n",
      "yahoo! 0\n",
      "   nmod 1\n",
      "style 0\n",
      "   compound 1\n",
      "burdensom 0\n",
      "   amod 1\n",
      "inflex 0\n",
      "   conj 1\n",
      "task 0\n",
      "   dobj 1\n",
      "train 0\n",
      "   compound 1\n",
      "re-prob 0\n",
      "   conj 1\n",
      "coarse-grain 0\n",
      "   amod 1\n",
      "url 0\n",
      "   compound 1\n",
      "health/med 0\n",
      "   amod 3\n",
      "   dep 1\n",
      "dmoz.org 0\n",
      "   compound 1\n",
      "engine 0\n",
      "   dobj 1\n",
      "channel 0\n",
      "   dep 1\n",
      "silicon 0\n",
      "   compound 1\n",
      "investor 0\n",
      "   compound 1\n",
      "financ 0\n",
      "   dep 1\n",
      "recip 0\n",
      "   compound 1\n",
      "   dep 1\n",
      "recipes2.alastra.com 0\n",
      "   compound 1\n",
      "b 0\n",
      "   compound 2\n",
      "   nmod 1\n",
      "comp.sys.mac.app 0\n",
      "   dep 1\n",
      "comp.sys.mac.advocaci 0\n",
      "   dep 1\n",
      "sci.phys 0\n",
      "   dep 1\n",
      "mixed45 0\n",
      "   dep 1\n",
      "mixed120 0\n",
      "   dep 1\n",
      "rec.sport.volleybal 0\n",
      "   compound 1\n",
      "rec.sport.cricket 0\n",
      "   dep 1\n",
      "rec.games.go 0\n",
      "   amod 1\n",
      "comp.lang.perl.misc 0\n",
      "   xcomp 1\n",
      "conclusions 0\n",
      "   dep 1\n",
      "amazon.com 0\n",
      "   ROOT 1\n",
      "   compound 1\n",
      "references 0\n",
      "   dep 1\n",
      "ariba 0\n",
      "   ROOT 1\n",
      "j. 0\n",
      "   compound 6\n",
      "m. 0\n",
      "   compound 9\n",
      "connel 0\n",
      "   appos 1\n",
      "   conj 1\n",
      "a. 0\n",
      "   compound 5\n",
      "du 0\n",
      "   conj 1\n",
      "automat 0\n",
      "   compound 1\n",
      "   amod 3\n",
      "languag 0\n",
      "   compound 1\n",
      "sigmod 0\n",
      "   ROOT 2\n",
      "p. 0\n",
      "   compound 6\n",
      "e. 0\n",
      "   compound 1\n",
      "caverle 0\n",
      "   ROOT 1\n",
      "l. 0\n",
      "   compound 5\n",
      "liu 0\n",
      "   appos 1\n",
      "   ROOT 2\n",
      "d. 0\n",
      "   compound 2\n",
      "rocco 0\n",
      "   conj 1\n",
      "git 0\n",
      "   appos 1\n",
      "cohen 0\n",
      "   ROOT 1\n",
      "w. 0\n",
      "   compound 6\n",
      "y. 0\n",
      "   compound 3\n",
      "singer 0\n",
      "   conj 1\n",
      "workshop 0\n",
      "   ROOT 1\n",
      "aaai 0\n",
      "   compound 1\n",
      "internet-bas 0\n",
      "   compound 1\n",
      "faq 0\n",
      "   ROOT 1\n",
      "apis 0\n",
      "   compound 1\n",
      "hawk 0\n",
      "   ROOT 1\n",
      "thistlewait 0\n",
      "   conj 1\n",
      "transact 0\n",
      "   ROOT 1\n",
      "ipeiroti 0\n",
      "   ROOT 2\n",
      "g. 0\n",
      "   compound 4\n",
      "sahami 0\n",
      "   conj 2\n",
      "hidden-web 0\n",
      "   amod 2\n",
      "qprober 0\n",
      "   ROOT 1\n",
      "tois 0\n",
      "   ROOT 1\n",
      "ngu 0\n",
      "   conj 1\n",
      "zeng 0\n",
      "   conj 1\n",
      "   ROOT 1\n",
      "qoscomput 0\n",
      "   ROOT 1\n",
      "dynam 0\n",
      "   amod 1\n",
      "www 0\n",
      "   nmod:poss 2\n",
      "   ROOT 1\n",
      "meng 0\n",
      "   ROOT 1\n",
      "   appos 1\n",
      "c. 0\n",
      "   compound 4\n",
      "t. 0\n",
      "   compound 1\n",
      "yu 0\n",
      "   appos 1\n",
      "   conj 1\n",
      "k.-l 0\n",
      "   conj 1\n",
      "coopis 0\n",
      "   ROOT 1\n",
      "net 0\n",
      "   ROOT 1\n",
      "node 0\n",
      "   ROOT 1\n",
      "nation 0\n",
      "   compound 1\n",
      "biotechnolog 0\n",
      "   compound 1\n",
      "papazoglou 0\n",
      "   ROOT 1\n",
      "concept 0\n",
      "   dep 1\n",
      "   compound 1\n",
      "wise 0\n",
      "   ROOT 2\n",
      "porter 0\n",
      "   ROOT 1\n",
      "f. 0\n",
      "   compound 1\n",
      "program 0\n",
      "   ROOT 1\n",
      "qiu 0\n",
      "   ROOT 1\n",
      "h.-p 0\n",
      "   conj 1\n",
      "frei 0\n",
      "   ROOT 1\n",
      "expans 0\n",
      "   ROOT 2\n",
      "concept-bas 0\n",
      "   amod 1\n",
      "sigir 0\n",
      "   ROOT 1\n",
      "   nmod 1\n",
      "pittsburgh 0\n",
      "   appos 1\n",
      "salton 0\n",
      "   ROOT 2\n",
      "buckley 0\n",
      "   conj 1\n",
      "term-weight 0\n",
      "   amod 1\n",
      "read 0\n",
      "   ROOT 1\n",
      "kauffman 0\n",
      "   ROOT 1\n",
      "morgan 0\n",
      "   compound 1\n",
      "san 0\n",
      "   compound 1\n",
      "francisco 0\n",
      "   appos 1\n",
      "ca 0\n",
      "   appos 1\n",
      "wong 0\n",
      "   appos 1\n",
      "yang 0\n",
      "   conj 1\n",
      "index 0\n",
      "   nmod 1\n",
      "cacm 0\n",
      "   ROOT 1\n",
      "schutz 0\n",
      "   ROOT 1\n",
      "o. 0\n",
      "   compound 2\n",
      "pedersen 0\n",
      "   conj 1\n",
      "thesaurus 0\n",
      "   ROOT 1\n",
      "cooccurrence-bas 0\n",
      "   amod 1\n",
      "sivashanmugam 0\n",
      "   ROOT 1\n",
      "k. 0\n",
      "   compound 2\n",
      "verma 0\n",
      "   appos 1\n",
      "sheth 0\n",
      "   conj 1\n",
      "icws 0\n",
      "   ROOT 1\n",
      "sreenath 0\n",
      "   ROOT 1\n",
      "r. 0\n",
      "   compound 1\n",
      "singh 0\n",
      "   conj 1\n",
      "agent-bas 0\n",
      "   amod 1\n",
      "jws 0\n",
      "   appos 1\n",
      "sugiura 0\n",
      "   ROOT 1\n",
      "etzioni 0\n",
      "   conj 1\n",
      "architectur 0\n",
      "   dep 1\n",
      "   ROOT 1\n",
      "w3c 0\n",
      "   compound 1\n",
      "februari 0\n",
      "   ROOT 1\n",
      "wang 0\n",
      "   ROOT 1\n",
      "metasearch 0\n",
      "   compound 1\n",
      "xu 0\n",
      "   ROOT 1\n",
      "b. 0\n",
      "   compound 2\n",
      "croft 0\n",
      "   conj 1\n",
      "local 0\n",
      "   amod 1\n",
      "benatallah 0\n",
      "   appos 1\n",
      "duma 0\n",
      "   conj 1\n",
      "kalagnanam 0\n",
      "   conj 1\n",
      "q. 0\n",
      "   compound 1\n",
      "z. 0\n",
      "   compound 1\n",
      "sheng 0\n",
      "   conj 1\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-ff5744ae3d20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'  '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print([(i['dependentGloss'], i['dep']) for i in sentence['basicDependencies']])\n",
    "\n",
    "def get_strength(d):\n",
    "    def get_count(k):\n",
    "        if k in d:\n",
    "            return d[k]\n",
    "        else:\n",
    "            return 0\n",
    "    return sum([get_count(k) for k in d.keys() if 'subj' in k])\n",
    "\n",
    "keys = sorted(dep_types_counter.keys(), key = lambda e : get_strength(dep_types_counter[e]), reverse=True)\n",
    "for w in keys:\n",
    "    print(w, get_strength(dep_types_counter[w]))\n",
    "    l = dep_types_counter[w].items()\n",
    "    for t, c in l:\n",
    "        print('  ', t, c)\n",
    "assert(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:33:06.481295Z",
     "start_time": "2018-06-05T12:33:01.458701Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model:  nus\n"
     ]
    }
   ],
   "source": [
    "import boto\n",
    "from gensim.models import Word2Vec\n",
    "#https://github.com/olivettigroup/materials-word-embeddings\n",
    "word2vec_pretrained = Word2Vec.load(\"../materials-word-embeddings/bin/word2vec_embeddings-SNAPSHOT.model\")\n",
    "\n",
    "word2vec_models_map = {}\n",
    "for dataset in storiesMap.keys():\n",
    "    infile = os.path.join('word2vec_models', dataset)\n",
    "    model = Word2Vec.load(infile)\n",
    "    word2vec_models_map[dataset] = model\n",
    "    print('loaded model: ', dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:33:14.427888Z",
     "start_time": "2018-06-05T12:33:14.408498Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed loading p model: nus\n"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "linear_models_map = {}\n",
    "for dataset in storiesMap.keys():\n",
    "#     outdir = 'MLP_C_100_train_0.8_edge_models++_dep_all_a=0.0005_t'\n",
    "    outdir = 'Linear_edge_models++_dep_all'\n",
    "    attention_key = 'L&D_w_20_top_1.00_L=non_neigbours'\n",
    "    clffile = os.path.join(outdir, attention_key + dataset + '.pkl')\n",
    "    try:\n",
    "        linear_models_map[dataset] = joblib.load(clffile)\n",
    "    except:\n",
    "        print('Failed loading model: %s' % dataset)\n",
    "        continue\n",
    "linear_p_models_map = {}\n",
    "for dataset in storiesMap.keys():\n",
    "    outdir = 'P_MODELS_pos_u_norm'\n",
    "    attention_key = 'L&D_w_20_top_1.00_L=non_neigbours'\n",
    "    clffile = os.path.join(outdir, attention_key + dataset + '.pkl')\n",
    "    try:\n",
    "        linear_p_models_map[dataset] = joblib.load(clffile)\n",
    "    except:\n",
    "        print('Failed loading p model: %s' % dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-05T12:38:53.358212Z",
     "start_time": "2018-06-05T12:33:24.860993Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUS\n",
      "Faild loading linear P model for dataset:  nus\n",
      "TEST  stories [37]\n",
      "[20, 35, 153, 79, 129, 82, 168, 138, 39, 159, 163, 80, 21, 18, 73, 13, 30, 108, 181, 155, 115, 173, 91, 54, 98, 135, 85, 162, 47, 40, 145, 100, 67, 45, 32, 105, 66]\n",
      "\n",
      "jump=pos,edge=Linear_train_0.8++_dep_all_a=0.0005_th=0.7\n",
      "jump=1,edge=Linear_train_0.8++_dep_all_a=0.0005_th=0.7\n",
      "jump=1,edge=1\n",
      "jump=pos,edge=1\n",
      "nus [story: 36/37] took: [00:13] remaining: [00:21:21]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mix type of y not allowed, got types {'continuous', 'binary'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-45d3d5cd66d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPRED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits)\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1421\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1422\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mix type of y not allowed, got types %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mys_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mlabel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mys_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Mix type of y not allowed, got types {'continuous', 'binary'}"
     ]
    }
   ],
   "source": [
    "weighted = False\n",
    "visualize = False\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "        \n",
    "import math\n",
    "import numpy as np\n",
    "import time\n",
    "import statistics\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.exception import NetworkXError\n",
    "from networkx.utils import not_implemented_for\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "import random\n",
    "\n",
    "class Weights:\n",
    "    LINEAR_MODEL_EDGE = \"jump=1,edge=Linear_train_0.8++_dep_all_a=0.0005_th=0.7\"\n",
    "    JUMP_POS_LINEAR_MODEL_EDGE = \"jump=pos,edge=Linear_train_0.8++_dep_all_a=0.0005_th=0.7\"\n",
    "    JUMP_POS = \"jump=pos,edge=1\"\n",
    "    EQUAL = \"jump=1,edge=1\"\n",
    "\n",
    "window_size = 6\n",
    "window_offset = math.ceil(window_size / 2)\n",
    "window_offsets = [\n",
    "    o for o in range(-window_offset, window_offset + 1) if o != 0\n",
    "]\n",
    "print_keys = True\n",
    "for dataset, stories in storiesMap.items():\n",
    "    print(dataset.upper())\n",
    "\n",
    "    try:\n",
    "        clf, normalizer_edge, names_edge = linear_models_map[dataset]\n",
    "    except:\n",
    "        print('Faild loading linear model for dataset: ', dataset)\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        clf_u, normalizer, names = linear_p_models_map[dataset]\n",
    "        print('p names', names)\n",
    "    except:\n",
    "        print('Faild loading linear P model for dataset: ', dataset)\n",
    "#         continue\n",
    "        \n",
    "    try:\n",
    "        word2vec_model = word2vec_models_map[dataset]\n",
    "    except:\n",
    "        print('Failed loading word2vec model for dataset', dataset)\n",
    "        continue\n",
    "\n",
    "    avg_time = 0\n",
    "    GR = []\n",
    "    PRED = []\n",
    "    \n",
    "    all_stories = [s.copy() for s in stories]\n",
    "    all_stories = sorted(all_stories, key = lambda e : str(e['suid']))\n",
    "    len_all = len(all_stories)\n",
    "    len_train = round(0.8 * len_all)\n",
    "    random.seed(10)\n",
    "    random.shuffle(all_stories)\n",
    "    \n",
    "    train_stories = all_stories[:len_train]\n",
    "    test_stories = all_stories[len_train:]\n",
    "    print('TEST  stories [%d]' % len(test_stories))\n",
    "    print([story['suid'] for story in test_stories])\n",
    "    print('')\n",
    "    \n",
    "    for story_idx, story in enumerate(test_stories):\n",
    "        if len(story['body']) == 0: continue\n",
    "        start_time = time.time()\n",
    "        processed = False\n",
    "\n",
    "        phrases = story['phrases']\n",
    "        stemmed_words = story['stemmed_words']\n",
    "        word_positions = story['word_positions']\n",
    "        word_levels = story['word_parsing_levels']\n",
    "        dep_types_counter = story['dep_types_counter']\n",
    "        \n",
    "        entities = story['entities']\n",
    "        gold_grams = set()\n",
    "        for entity in entities:\n",
    "            grams = stanford_core.word_tokenize(entity['id'])\n",
    "            grams = [stem(token).lower() for token in grams]\n",
    "            gold_grams.update(grams)\n",
    "            \n",
    "        # compute feature pre-reqs\n",
    "        word2vec = {}\n",
    "        for i, word in enumerate(stemmed_words):\n",
    "            if word in word2vec.keys() or word not in word2vec_model.wv.vocab:\n",
    "                continue\n",
    "            w_embed = word2vec_model.wv.get_vector(word)\n",
    "            word2vec[word] = w_embed\n",
    "        count = {}\n",
    "        pos = {}\n",
    "        first_pos = {}\n",
    "        \n",
    "        def update_or_insert(d, k, v):\n",
    "            if k in d.keys():\n",
    "                d[k] += v\n",
    "            else:\n",
    "                d[k] = v\n",
    "\n",
    "        for i, word in enumerate(stemmed_words):\n",
    "            update_or_insert(count, word, 1)\n",
    "            update_or_insert(pos, word, 1 / (i + 1))\n",
    "            if not word in first_pos:\n",
    "                first_pos[word] = 1/(i+1)\n",
    "                \n",
    "            \n",
    "        def get_part_strength(d, key):\n",
    "            def get_count(k):\n",
    "                if k in d:\n",
    "                    return d[k]\n",
    "                else:\n",
    "                    return 0\n",
    "            s = sum([get_count(k) for k in d.keys() if key in k.lower()])\n",
    "            return s\n",
    "        \n",
    "        def compose_features(u, v, H, H_first):\n",
    "            count_u = count[u]\n",
    "            count_v = count[v]\n",
    "            pos_u = pos[u]\n",
    "            pos_v = pos[v]\n",
    "            wvec_u = word2vec[u]\n",
    "            wvec_v = word2vec[v]\n",
    "            u_level = 1 - 1 / np.mean(word_levels[u])\n",
    "            v_level = 1 - 1 / np.mean(word_levels[v])\n",
    "            s_uv = word2vec_model.wv.similarity(u, v)\n",
    "            \n",
    "#             parts = [\n",
    "#                 'subj',\n",
    "#                 'obj',\n",
    "#                 'appos',\n",
    "#                 'conj',\n",
    "#                 'compound',\n",
    "#                 'mod',\n",
    "#                 'det',\n",
    "#                 ]                        \n",
    "#             # part of sentence\n",
    "            parts = [\n",
    "                'subj',\n",
    "                'obj',\n",
    "                'appos',\n",
    "                'conj',\n",
    "                'compound',\n",
    "                'mod',\n",
    "                'acl',\n",
    "                'det',\n",
    "                'neg',\n",
    "                'mark',\n",
    "                'auxpass',\n",
    "                'ccomp',\n",
    "                'mwe',\n",
    "                'parataxis',\n",
    "                'cop',\n",
    "                'advcl',\n",
    "                'cc',\n",
    "                'discourse',\n",
    "                'xcomp',\n",
    "                'case',\n",
    "                'dep',\n",
    "                'aux',\n",
    "                'punct',\n",
    "                'root',\n",
    "            ]\n",
    "#             parts = [\n",
    "#                 'csubj',\n",
    "#                 'nsubj',\n",
    "#                 'nsubjpass',\n",
    "#                 'csubjpass',\n",
    "#                 'dobj',\n",
    "#                 'iobj',\n",
    "#                 'appos',\n",
    "#                 'conj',\n",
    "#                 'cc:preconj',\n",
    "#                 'compound',\n",
    "#                 'compound:prt',\n",
    "#                 'nmod',\n",
    "#                 'advmod',\n",
    "#                 'nummod',\n",
    "#                 'nmod:tmod',\n",
    "#                 'nmod:npmod',\n",
    "#                 'nmod:poss',\n",
    "#                 'amod',\n",
    "#                 'acl',\n",
    "#                 'acl:relcl',\n",
    "#                 'det',\n",
    "#                 'det:predet',\n",
    "#                 'neg',\n",
    "#                 'mark',\n",
    "#                 'auxpass',\n",
    "#                 'ccomp',\n",
    "#                 'mwe',\n",
    "#                 'parataxis',\n",
    "#                 'cop',\n",
    "#                 'advcl',\n",
    "#                 'cc',\n",
    "#                 'discourse',\n",
    "#                 'xcomp',\n",
    "#                 'case',\n",
    "#                 'dep',\n",
    "#                 'root',\n",
    "#                 'aux',\n",
    "#                 'punct',\n",
    "#             ]\n",
    "            v_parts = np.array([get_part_strength(dep_types_counter[v], part) for part in parts])\n",
    "            u_parts = np.array([get_part_strength(dep_types_counter[u], part) for part in parts])\n",
    "\n",
    "            # co-occurence count\n",
    "            co_occur = H[u][v]['weight']\n",
    "\n",
    "            # first time together\n",
    "            co_first = H_first[u][v]['weight']\n",
    "\n",
    "            # neighbours\n",
    "            u_neigs = list(H[u].keys())\n",
    "            v_neigs = list(H[v].keys())\n",
    "            uv_neigs = [u for u in u_neigs if u in v_neigs]\n",
    "\n",
    "            x = np.concatenate(\n",
    "                (np.array([\n",
    "                    count_u, count_v, pos_u, pos_v, u_level, v_level, s_uv,\n",
    "                    co_occur,\n",
    "                    co_first,\n",
    "                    len(u_neigs),\n",
    "                    len(v_neigs),\n",
    "                    len(uv_neigs),\n",
    "                ]), u_parts, v_parts, wvec_u, wvec_v),\n",
    "                axis=0)\n",
    "            x = np.expand_dims(x, axis=0)\n",
    "            return x\n",
    "\n",
    "        def valid_token(u, verbose=False):\n",
    "            if u not in word_levels.keys():\n",
    "                if verbose:\n",
    "                    print(u, 'not in word_levels')\n",
    "                return False\n",
    "            if u not in word2vec_model.wv.vocab:\n",
    "                if verbose:\n",
    "                    print(u, 'not in vocabulary')\n",
    "                return False\n",
    "            return True\n",
    "        \n",
    "        H_first = nx.Graph()\n",
    "        H_all = nx.Graph()\n",
    "        for i, v in enumerate(stemmed_words):\n",
    "            if len(v) <= 1 or not valid_token(v):\n",
    "                continue\n",
    "            for offset in window_offsets:\n",
    "                idx = i + offset\n",
    "                if idx < len(stemmed_words) and idx >= 0:\n",
    "                    u = stemmed_words[idx]\n",
    "                    if u == v:\n",
    "                        continue\n",
    "                    if len(u) <= 1 or not valid_token(u):\n",
    "                        continue\n",
    "                    if H_all.has_edge(u, v):\n",
    "                        H_all[u][v]['weight'] += 1\n",
    "                    else:\n",
    "                        H_all.add_edge(u, v, weight=1)\n",
    "                        assert(H_first.has_edge(u, v) == False)\n",
    "                        H_first.add_edge(u, v, weight=1/(i+1))\n",
    "        P_all = {}\n",
    "        for v in H_all:\n",
    "            if v not in P_all.keys():\n",
    "                P_all[v] = pos[v]\n",
    "                \n",
    "        # create edges       \n",
    "        H = nx.DiGraph()\n",
    "        for i, v in enumerate(stemmed_words):\n",
    "            if len(v) <= 1 or not valid_token(v):\n",
    "                continue\n",
    "            for offset in window_offsets:\n",
    "                idx = i + offset\n",
    "                if idx < len(stemmed_words) and idx >= 0:\n",
    "                    u = stemmed_words[idx]\n",
    "                    if u == v:\n",
    "                        continue\n",
    "                    if len(u) <= 1 or not valid_token(u):\n",
    "                        continue\n",
    "                    if H.has_edge(u, v) and H.has_edge(v, u):\n",
    "                        continue                   \n",
    "                    uv = compose_features(u, v, H_all, H_first)\n",
    "                    uv = normalizer_edge.transform(uv)\n",
    "                    uv_s = float(clf.predict(uv))\n",
    "                    if uv_s >= 0.7:\n",
    "                        H.add_edge(u, v, weight=uv_s)\n",
    "                    vu = compose_features(v, u, H_all, H_first)\n",
    "                    vu = normalizer_edge.transform(vu)\n",
    "                    vu_s = float(clf.predict(vu))\n",
    "                    if vu_s >= 0.7:\n",
    "                        H.add_edge(v, u, weight=vu_s)\n",
    "                    GR.append(int(v in gold_grams))\n",
    "                    GR.append(int(u in gold_grams))\n",
    "                    PRED.append(uv_s)\n",
    "                    PRED.append(vu_s)\n",
    "                                 \n",
    "        P = {}\n",
    "        for v in H:\n",
    "            if v not in P.keys():\n",
    "                P[v] = pos[v]\n",
    "            for u in H[v]:\n",
    "                if u not in P.keys():\n",
    "                    P[u] = pos[u]\n",
    "                \n",
    "#         def compose_features_u(u):\n",
    "#             count_u = count[u]\n",
    "#             pos_u = pos[u]\n",
    "#             fst_pos = first_pos[u]\n",
    "#             wvec_u = word2vec[u]\n",
    "#             u_level = np.mean(word_levels[u])\n",
    "#             features_map = {\n",
    "#                 'pos_u'   : pos_u,\n",
    "#                 'u_level' : 1 - 1/u_level,\n",
    "#                 'count_u' : count_u,\n",
    "#                 'first_pos' : fst_pos\n",
    "#             }\n",
    "#             x = np.array([features_map[name] for name in names])\n",
    "# #             x = np.concatenate((x, wvec_u), axis=0)\n",
    "#             x = np.expand_dims(x, axis=0)\n",
    "#             return x\n",
    "#         P = {}\n",
    "#         for idx, v in enumerate(stemmed_words):\n",
    "#             if len(v) <= 1 or not valid_token(v) or v in P.keys():\n",
    "#                 continue\n",
    "#             x = compose_features_u(v)\n",
    "#             x = normalizer.transform(x)\n",
    "#             score = clf_u.predict(x)\n",
    "#             P[v] = float(score)\n",
    "#         min_w = min(P.values())\n",
    "#         max_w = max(P.values())\n",
    "#         if max_w <= 0:\n",
    "#             print('all zero -- sidx %d, min-w: %f, max-w: %f' %(story_idx, min_w, max_w))\n",
    "#         for k, v in P.items():\n",
    "#             if max_w <= 0:\n",
    "#                 P[k] = v - min_w\n",
    "#             else:\n",
    "#                 P[k] = max(0, P[k])\n",
    "#         if visualize:\n",
    "#             plt.clf()\n",
    "#             s = sum(pos.values())\n",
    "#             for k, v in pos.items():\n",
    "#                 pos[k] = v / s\n",
    "#             plt.hist(pos.values())\n",
    "#             plt.title('pos')\n",
    "#             plt.show()\n",
    "             \n",
    "#             plt.clf()\n",
    "#             plt.hist(P.values())\n",
    "#             plt.title('pred')\n",
    "#             plt.show()\n",
    "            \n",
    "#             plt.clf()\n",
    "#             s = sum(P.values())\n",
    "#             for k, v in P.items():\n",
    "#                 P[k] = v / s\n",
    "#             plt.hist(P.values())\n",
    "#             plt.title('pred norm')\n",
    "#             plt.show()\n",
    "#             assert(False)\n",
    "\n",
    "        for (weights_type) in [\n",
    "                Weights.JUMP_POS_LINEAR_MODEL_EDGE,\n",
    "                Weights.LINEAR_MODEL_EDGE,\n",
    "                Weights.EQUAL,\n",
    "                Weights.JUMP_POS\n",
    "        ]:\n",
    "            algo = str(weights_type)\n",
    "            if print_keys:\n",
    "                print(algo)\n",
    "            FOLDER = os.path.join('./output/', dataset, algo)\n",
    "            if not os.path.exists(FOLDER): os.makedirs(FOLDER)\n",
    "            keyphraseFile = os.path.join(FOLDER, str(story['suid']))\n",
    "            processed = True\n",
    "\n",
    "            if weights_type == Weights.LINEAR_MODEL_EDGE:\n",
    "                calculated_page_rank = nx.pagerank(H)\n",
    "            elif weights_type == Weights.JUMP_POS_LINEAR_MODEL_EDGE:\n",
    "                calculated_page_rank = nx.pagerank(H, personalization=P)\n",
    "            else:\n",
    "                if weights_type == Weights.JUMP_POS:\n",
    "                    calculated_page_rank = nx.pagerank(H_all, personalization=P_all)\n",
    "                else:\n",
    "                    calculated_page_rank = nx.pagerank(H_all)\n",
    "\n",
    "            # reconstruct phrases\n",
    "            def getWordScore(word):\n",
    "                try:\n",
    "                    return calculated_page_rank[word]\n",
    "                except KeyError as e:\n",
    "                    #  print('key error: ', word)\n",
    "                    pass\n",
    "                return 0\n",
    "\n",
    "            phrase_scores = set()\n",
    "            for phrase in phrases:\n",
    "                phrase_words = [\n",
    "                    stem(word).lower() for word in phrase.split(\"_\")\n",
    "                ]\n",
    "                phrase_score = sum(\n",
    "                    [getWordScore(word) for word in phrase_words])\n",
    "                if phrase_score > 0:\n",
    "                    stemmed_phrase = ' '.join(phrase_words)\n",
    "                    phrase_scores.add((stemmed_phrase, phrase_score))\n",
    "\n",
    "            # single words\n",
    "            phrase_scores.update(calculated_page_rank.items())\n",
    "\n",
    "            sorted_phrases = sorted(\n",
    "                phrase_scores, key=lambda e: e[1], reverse=True)\n",
    "\n",
    "            keyphraseFile = open(keyphraseFile, 'w')\n",
    "            for phrase, score in sorted_phrases:\n",
    "                keyphraseFile.write(phrase + ':::' + str(score) + '\\n')\n",
    "            keyphraseFile.close()\n",
    "\n",
    "        if processed:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            avg_time = (avg_time * story_idx + elapsed_time) / (story_idx + 1)\n",
    "            elapsed = time.strftime('[%M:%S]', time.gmtime(elapsed_time))\n",
    "            remaining = time.strftime(\n",
    "                '[%H:%M:%S]',\n",
    "                time.gmtime(avg_time * (len(stories) - story_idx - 1)))\n",
    "            sys.stdout.write(\"\\r%s [story: %d/%d] took: %s remaining: %s\" %\n",
    "                             (dataset, story_idx, len(test_stories), elapsed,\n",
    "                              remaining))\n",
    "        print_keys = False\n",
    "    \n",
    "    print('')\n",
    "    print(classification_report(GR, PRED))\n",
    "    print('')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
